package mcpagent

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/manishiitg/multi-llm-provider-go/llmtypes"

	"github.com/mark3labs/mcp-go/mcp"

	"mcpagent/agent/codeexec"
	"mcpagent/agent/prompt"
	"mcpagent/events"
	"mcpagent/llm"
	loggerv2 "mcpagent/logger/v2"
	"mcpagent/mcpcache"
	"mcpagent/mcpcache/codegen"
	"mcpagent/mcpclient"
	"mcpagent/observability"
)

// CustomTool represents a custom tool with its definition and execution function
type CustomTool struct {
	Definition llmtypes.Tool
	Execution  func(ctx context.Context, args map[string]interface{}) (string, error)
	Category   string // Tool category (e.g., "workspace", "human", "virtual", "custom", etc.)
}

// AgentEventListener defines the interface for event listeners
type AgentEventListener interface {
	HandleEvent(ctx context.Context, event *events.AgentEvent) error
	Name() string
}

// AgentMode defines the type of agent behavior
type AgentMode string

const (
	// SimpleAgent is the standard tool-using agent without explicit reasoning
	SimpleAgent AgentMode = "simple"
)

// AgentOption defines a functional option for configuring an Agent
type AgentOption func(*Agent)

// WithMode sets the agent mode
func WithMode(mode AgentMode) AgentOption {
	return func(a *Agent) {
		a.AgentMode = mode
	}
}

// WithLogger sets a custom logger
func WithLogger(logger loggerv2.Logger) AgentOption {
	return func(a *Agent) {
		a.Logger = logger
	}
}

// WithTracer sets a tracer for observability
// The tracer will be wrapped in a StreamingTracer and added to the Tracers slice
func WithTracer(tracer observability.Tracer) AgentOption {
	return func(a *Agent) {
		if tracer != nil {
			// Create streaming tracer that wraps the base tracer
			streamingTracer := NewStreamingTracer(tracer, 100)
			// Add to tracers slice
			a.Tracers = append(a.Tracers, streamingTracer)
		}
	}
}

// WithTraceID sets the trace ID for observability
func WithTraceID(traceID observability.TraceID) AgentOption {
	return func(a *Agent) {
		a.TraceID = traceID
	}
}

// WithProvider sets the LLM provider
func WithProvider(provider llm.Provider) AgentOption {
	return func(a *Agent) {
		a.provider = provider
	}
}

// WithMaxTurns sets the maximum conversation turns
func WithMaxTurns(maxTurns int) AgentOption {
	return func(a *Agent) {
		a.MaxTurns = maxTurns
	}
}

// WithTemperature sets the LLM temperature
func WithTemperature(temperature float64) AgentOption {
	return func(a *Agent) {
		a.Temperature = temperature
	}
}

// WithToolChoice sets the tool choice strategy
func WithToolChoice(toolChoice string) AgentOption {
	return func(a *Agent) {
		a.ToolChoice = toolChoice
	}
}

// WithLargeOutputVirtualTools enables/disables context offloading virtual tools
// When enabled, large tool outputs are automatically offloaded to filesystem (offload context pattern)
func WithLargeOutputVirtualTools(enabled bool) AgentOption {
	return func(a *Agent) {
		a.EnableLargeOutputVirtualTools = enabled
	}
}

// WithLargeOutputThreshold sets the token threshold for context offloading
// When tool outputs exceed this threshold (in tokens), they are offloaded to filesystem (offload context pattern)
// Default is 10000 tokens if not set
// Note: The threshold is compared against token count using tiktoken encoding, not character count
func WithLargeOutputThreshold(threshold int) AgentOption {
	return func(a *Agent) {
		a.LargeOutputThreshold = threshold
	}
}

// WithContextSummarization enables/disables context summarization
// When enabled and SummarizeOnMaxTurns is true, conversation history is summarized when max turns is reached
func WithContextSummarization(enabled bool) AgentOption {
	return func(a *Agent) {
		a.EnableContextSummarization = enabled
		if enabled {
			a.SummarizeOnMaxTurns = true // Default to true when enabled
		}
	}
}

// WithSummarizeOnMaxTurns enables/disables summarization when max turns is reached
// Requires EnableContextSummarization to be true
func WithSummarizeOnMaxTurns(enabled bool) AgentOption {
	return func(a *Agent) {
		a.SummarizeOnMaxTurns = enabled
	}
}

// WithSummaryKeepLastMessages sets the number of recent messages to keep when summarizing
// Default is 8 messages (roughly 3-4 turns)
func WithSummaryKeepLastMessages(count int) AgentOption {
	return func(a *Agent) {
		a.SummaryKeepLastMessages = count
	}
}

// WithToolTimeout sets the tool execution timeout
func WithToolTimeout(timeout time.Duration) AgentOption {
	return func(a *Agent) {
		a.ToolTimeout = timeout
	}
}

// WithCustomTools adds custom tools to the agent during creation
func WithCustomTools(tools []llmtypes.Tool) AgentOption {
	return func(a *Agent) {
		a.Tools = append(a.Tools, tools...)
	}
}

// WithSmartRouting enables/disables smart routing for tool filtering
func WithSmartRouting(enabled bool) AgentOption {
	return func(a *Agent) {
		a.EnableSmartRouting = enabled
	}
}

// WithSmartRoutingThresholds sets custom thresholds for smart routing
func WithSmartRoutingThresholds(maxTools, maxServers int) AgentOption {
	return func(a *Agent) {
		a.SmartRoutingThreshold.MaxTools = maxTools
		a.SmartRoutingThreshold.MaxServers = maxServers
	}
}

// WithSmartRoutingConfig sets additional smart routing configuration
func WithSmartRoutingConfig(temperature float64, maxTokens, maxMessages, userMsgLimit, assistantMsgLimit int) AgentOption {
	return func(a *Agent) {
		a.SmartRoutingConfig.Temperature = temperature
		a.SmartRoutingConfig.MaxTokens = maxTokens
		a.SmartRoutingConfig.MaxMessages = maxMessages
		a.SmartRoutingConfig.UserMsgLimit = userMsgLimit
		a.SmartRoutingConfig.AssistantMsgLimit = assistantMsgLimit
	}
}

// WithSystemPrompt sets a custom system prompt
func WithSystemPrompt(systemPrompt string) AgentOption {
	return func(a *Agent) {
		a.SystemPrompt = systemPrompt
		a.hasCustomSystemPrompt = true
	}
}

// WithDiscoverResource enables/disables resource discovery in system prompt
func WithDiscoverResource(enabled bool) AgentOption {
	return func(a *Agent) {
		a.DiscoverResource = enabled
	}
}

// WithDiscoverPrompt enables/disables prompt discovery in system prompt
func WithDiscoverPrompt(enabled bool) AgentOption {
	return func(a *Agent) {
		a.DiscoverPrompt = enabled
	}
}

// WithCrossProviderFallback sets the cross-provider fallback configuration
func WithCrossProviderFallback(crossProviderFallback *CrossProviderFallback) AgentOption {
	return func(a *Agent) {
		a.CrossProviderFallback = crossProviderFallback
	}
}

// WithSelectedTools sets specific tools to use (format: "server:tool")
func WithSelectedTools(tools []string) AgentOption {
	return func(a *Agent) {
		a.selectedTools = tools
	}
}

// WithSelectedServers sets the selected servers list
func WithSelectedServers(servers []string) AgentOption {
	return func(a *Agent) {
		// Store selected servers for tool filtering logic
		// This is used to determine which servers should use "all tools" mode
		a.selectedServers = servers
	}
}

// WithCodeExecutionMode enables/disables code execution mode
// When enabled: Only virtual tools (discover_code_files, write_code) are exposed to the LLM
// MCP tools and custom tools are NOT added directly - LLM must use generated Go code via write_code
// When disabled (default): All MCP tools are added directly as LLM tools
func WithCodeExecutionMode(enabled bool) AgentOption {
	return func(a *Agent) {
		a.UseCodeExecutionMode = enabled
	}
}

// WithDisableCache disables MCP server connection caching
// When enabled: Skips cache lookup and always performs fresh connections
// When disabled (default): Uses cache to speed up connection establishment
func WithDisableCache(disable bool) AgentOption {
	return func(a *Agent) {
		a.DisableCache = disable
	}
}

// WithServerName sets the server name(s) to connect to
// Default: AllServers (connects to all configured servers)
// Can be a single server name, comma-separated list, or mcpclient.AllServers
func WithServerName(serverName string) AgentOption {
	return func(a *Agent) {
		a.serverName = serverName
	}
}

// Agent wraps MCP clients, an LLM, and an observability tracer to answer questions using tool calls.
// It is generic enough to be reused by CLI commands, services, or tests.
type Agent struct {
	// Context for cancellation and lifecycle management
	ctx context.Context

	// Legacy single client (first in the list) kept for backward compatibility
	Client mcpclient.ClientInterface

	// NEW: multiple clients keyed by server name
	Clients map[string]mcpclient.ClientInterface

	// Map tool name â†’ server name (quick dispatch)
	toolToServer map[string]string

	LLM     llmtypes.Model
	Tracers []observability.Tracer // Support multiple tracers
	Tools   []llmtypes.Tool

	// Configuration knobs
	MaxTurns        int
	Temperature     float64
	ToolChoice      string
	ModelID         string
	AgentMode       AgentMode     // NEW: Agent mode (Simple or ReAct)
	ToolTimeout     time.Duration // Tool execution timeout (default: 5 minutes)
	selectedTools   []string      // Selected tools in "server:tool" format
	selectedServers []string      // Selected servers list for "all tools" mode determination
	toolFilter      *ToolFilter   // Unified tool filter for consistent filtering

	// Enhanced tracking info
	SystemPrompt string
	TraceID      observability.TraceID
	configPath   string // Path to MCP config file for on-demand connections
	serverName   string // Server name(s) to connect to (default: AllServers)

	// cached list of server names (for metadata convenience)
	servers []string

	// Event system for observability - REMOVED: No longer using event dispatchers

	// Provider information
	provider llm.Provider

	// Context offloading: handles offloading large tool outputs to filesystem
	toolOutputHandler *ToolOutputHandler

	// Context offloading configuration: enables virtual tools for accessing offloaded outputs
	EnableLargeOutputVirtualTools bool

	// Context offloading threshold: custom threshold for when to offload tool outputs (0 = use default)
	LargeOutputThreshold int

	// Context summarization configuration (see context_summarization.go)
	EnableContextSummarization bool // Enable context summarization feature
	SummarizeOnMaxTurns        bool // Summarize when max turns is reached
	SummaryKeepLastMessages    int  // Number of recent messages to keep when summarizing (0 = use default)

	// Store prompts and resources for system prompt rebuilding
	prompts   map[string][]mcp.Prompt
	resources map[string][]mcp.Resource

	// Flag to track if a custom system prompt was provided
	hasCustomSystemPrompt bool

	// Custom tools that are handled as virtual tools
	customTools map[string]CustomTool

	// Custom logger (optional) - uses v2.Logger interface
	Logger loggerv2.Logger

	// Listeners for typed events
	listeners []AgentEventListener
	mu        sync.RWMutex

	// Smart routing configuration with defaults
	EnableSmartRouting    bool
	SmartRoutingThreshold struct {
		MaxTools   int
		MaxServers int
	}

	// Smart routing configuration for additional parameters
	SmartRoutingConfig struct {
		Temperature       float64
		MaxTokens         int
		MaxMessages       int
		UserMsgLimit      int
		AssistantMsgLimit int
	}

	// Pre-filtered tools for smart routing (determined once at conversation start)
	filteredTools []llmtypes.Tool

	// NEW: Track appended system prompts separately for smart routing
	AppendedSystemPrompts []string // Track each appended prompt
	OriginalSystemPrompt  string   // Keep original system prompt
	HasAppendedPrompts    bool     // Flag to indicate if any prompts were appended

	// Hierarchy tracking fields for event tree structure
	currentParentEventID  string // Track current parent event ID
	currentHierarchyLevel int    // Track current hierarchy level (0=root, 1=child, etc.)

	// Resource discovery configuration
	DiscoverResource bool // If true, include resource details in system prompt (default: true)

	// Prompt discovery configuration
	DiscoverPrompt bool // If true, include prompt details in system prompt (default: true)

	// Code execution mode configuration
	// When enabled: Only virtual tools (discover_code_files, write_code) are exposed to the LLM
	// MCP tools and custom tools are NOT added directly - LLM must use generated Go code via write_code
	// When disabled (default): All MCP tools are added directly as LLM tools
	UseCodeExecutionMode bool

	// Cache configuration
	// When enabled: Skips cache lookup and always performs fresh connections
	// When disabled (default): Uses cache to speed up connection establishment (60-85% faster)
	DisableCache bool

	// Folder guard paths for code execution mode
	// These paths are validated at AST level before code execution
	FolderGuardReadPaths  []string // Paths allowed for read operations
	FolderGuardWritePaths []string // Paths allowed for write operations

	// Cross-provider fallback configuration
	CrossProviderFallback *CrossProviderFallback // Cross-provider fallback configuration from frontend

	// API keys for providers (used for fallback LLM creation)
	APIKeys *AgentAPIKeys

	// Cumulative token tracking for entire conversation
	cumulativePromptTokens     int          // Cumulative prompt/input tokens
	cumulativeCompletionTokens int          // Cumulative completion/output tokens
	cumulativeTotalTokens      int          // Cumulative total tokens
	cumulativeCacheTokens      int          // Cumulative cache tokens (sum of all cache-related tokens)
	cumulativeReasoningTokens  int          // Cumulative reasoning tokens (for models like o3)
	cumulativeCacheDiscount    float64      // Sum of cache discounts (for averaging)
	llmCallCount               int          // Number of LLM calls made
	cacheEnabledCallCount      int          // Number of calls with cache tokens > 0
	tokenTrackingMutex         sync.RWMutex // Mutex for thread-safe token accumulation
}

// CrossProviderFallback represents cross-provider fallback configuration
type CrossProviderFallback struct {
	Provider string   `json:"provider"`
	Models   []string `json:"models"`
}

// AgentAPIKeys holds API keys for different providers (for Agent struct)
type AgentAPIKeys struct {
	OpenRouter *string
	OpenAI     *string
	Anthropic  *string
	Vertex     *string
	Bedrock    *AgentBedrockConfig
}

// AgentBedrockConfig holds Bedrock-specific configuration (for Agent struct)
type AgentBedrockConfig struct {
	Region string
}

// GetProvider returns the provider
func (a *Agent) GetProvider() llm.Provider {
	return a.provider
}

// GetToolOutputHandler returns the tool output handler
func (a *Agent) GetToolOutputHandler() *ToolOutputHandler {
	return a.toolOutputHandler
}

// GetPrompts returns the prompts map
func (a *Agent) GetPrompts() map[string][]mcp.Prompt {
	return a.prompts
}

// GetResources returns the resources map
func (a *Agent) GetResources() map[string][]mcp.Resource {
	return a.resources
}

// GetToolToServer returns the tool to server mapping
func (a *Agent) GetToolToServer() map[string]string {
	return a.toolToServer
}

// SetProvider sets the provider
func (a *Agent) SetProvider(provider llm.Provider) {
	a.provider = provider
}

// SetToolOutputHandler sets the tool output handler
func (a *Agent) SetToolOutputHandler(handler *ToolOutputHandler) {
	a.toolOutputHandler = handler
}

// SetFolderGuardPaths sets the folder guard paths for code execution validation
// readPaths: paths allowed for read operations (workspace_tools read functions)
// writePaths: paths allowed for write operations (workspace_tools write functions)
func (a *Agent) SetFolderGuardPaths(readPaths, writePaths []string) {
	a.FolderGuardReadPaths = readPaths
	a.FolderGuardWritePaths = writePaths
	if a.Logger != nil {
		a.Logger.Info("ðŸ”’ [CODE_EXECUTION] Folder guard paths set",
			loggerv2.Any("read_paths", readPaths),
			loggerv2.Any("write_paths", writePaths))
	}
}

// GetFolderGuardPaths returns the folder guard paths
func (a *Agent) GetFolderGuardPaths() (readPaths, writePaths []string) {
	return a.FolderGuardReadPaths, a.FolderGuardWritePaths
}

// extractModelIDFromLLM extracts the model ID from the LLM instance
// Returns the model ID from llm.GetModelID(), or "unknown" if empty
//
// GetModelID() is now part of the llmtypes.Model interface, so all implementations
// must provide it. This makes the extraction straightforward and type-safe.
func extractModelIDFromLLM(llm llmtypes.Model) string {
	modelID := llm.GetModelID()
	if modelID == "" {
		return "unknown"
	}
	return modelID
}

// NewAgent creates a new Agent with the given options
// The modelID is automatically extracted from the LLM instance
// By default, connects to all servers (AllServers). Use WithServerName() to filter.
func NewAgent(ctx context.Context, llm llmtypes.Model, configPath string, options ...AgentOption) (*Agent, error) {
	if llm == nil {
		return nil, fmt.Errorf("LLM cannot be nil")
	}

	// Extract model ID from LLM instance
	// This ensures the modelID matches the actual LLM being used
	modelID := extractModelIDFromLLM(llm)

	// Create agent with default values
	ag := &Agent{
		ctx:                           ctx,
		LLM:                           llm,
		Tracers:                       []observability.Tracer{},        // Default: empty tracers array
		MaxTurns:                      GetDefaultMaxTurns(SimpleAgent), // Default to simple mode
		Temperature:                   0.0,                             // Default temperature
		ToolChoice:                    "auto",                          // Default tool choice
		ModelID:                       modelID,
		AgentMode:                     SimpleAgent,                 // Default to simple mode
		TraceID:                       "",                          // Default: empty trace ID
		provider:                      "",                          // Will be set by caller
		EnableLargeOutputVirtualTools: true,                        // Default to enabled
		LargeOutputThreshold:          0,                           // Default: 0 means use default threshold (10000)
		EnableContextSummarization:    false,                       // Default to disabled
		SummarizeOnMaxTurns:           false,                       // Default to disabled
		SummaryKeepLastMessages:       0,                           // Default: 0 means use default (8 messages)
		Logger:                        loggerv2.NewDefault(),       // Default logger
		customTools:                   make(map[string]CustomTool), // Initialize custom tools map

		// Smart routing configuration with defaults
		EnableSmartRouting: false, // Default to disabled for now
		SmartRoutingThreshold: struct {
			MaxTools   int
			MaxServers int
		}{
			MaxTools:   30, // Default threshold
			MaxServers: 4,  // Default threshold
		},
		// Smart routing configuration for additional parameters
		SmartRoutingConfig: struct {
			Temperature       float64
			MaxTokens         int
			MaxMessages       int
			UserMsgLimit      int
			AssistantMsgLimit int
		}{
			Temperature:       0.1,  // Default temperature for routing
			MaxTokens:         5000, // Default max tokens for routing
			MaxMessages:       8,    // Default max conversation messages
			UserMsgLimit:      200,  // Default user message character limit
			AssistantMsgLimit: 300,  // Default assistant message character limit
		},

		// Initialize hierarchy tracking fields
		currentParentEventID:  "", // Start with no parent
		currentHierarchyLevel: 0,  // Start at root level

		// Initialize resource discovery (default: true - include resources in system prompt)
		DiscoverResource: true,

		// Initialize prompt discovery (default: true - include prompts in system prompt)
		DiscoverPrompt: true,

		// Initialize cache (default: false - caching enabled by default)
		DisableCache: false,

		// Initialize server name (default: AllServers - connect to all servers)
		serverName: mcpclient.AllServers,
	}

	// Apply all options
	for _, option := range options {
		option(ag)
	}

	// Use logger from options (or default if not set)
	logger := ag.Logger
	if logger == nil {
		logger = loggerv2.NewDefault()
		ag.Logger = logger
	}

	// Use serverName from options (or default AllServers)
	serverName := ag.serverName
	if serverName == "" {
		serverName = mcpclient.AllServers
	}

	logger.Info("NewAgent started", loggerv2.String("config_path", configPath))

	// Load merged MCP servers configuration (base + user)
	config, err := mcpclient.LoadMergedConfig(configPath, logger)
	if err != nil {
		return nil, fmt.Errorf("failed to load merged MCP config: %w", err)
	}

	logger.Debug("Merged config contains servers", loggerv2.Int("server_count", len(config.MCPServers)))
	for name := range config.MCPServers {
		logger.Debug("Server found", loggerv2.String("server_name", name))
	}

	if modelID == "unknown" {
		logger.Warn("Could not extract model ID from LLM instance, using 'unknown'",
			loggerv2.String("fallback", "unknown"))
	}

	// Enable code generation in cache manager if code execution mode is enabled
	// This ensures MCP server code is only generated when needed
	cacheManager := mcpcache.GetCacheManager(logger)
	cacheManager.SetCodeGenerationEnabled(ag.UseCodeExecutionMode)

	clients, toolToServer, allLLMTools, servers, prompts, resources, systemPrompt, err := NewAgentConnection(ctx, llm, serverName, configPath, string(ag.TraceID), ag.Tracers, logger, ag.DisableCache)

	if err != nil {
		logger.Error("NewAgentConnection failed", err)
		return nil, err
	}

	// Use first client for legacy compatibility
	var firstClient mcpclient.ClientInterface
	if len(clients) > 0 {
		for _, c := range clients {
			firstClient = c
			break
		}
	}

	// Initialize tool output handler
	toolOutputHandler := NewToolOutputHandler()

	// Apply custom threshold if set via WithLargeOutputThreshold option
	if ag.LargeOutputThreshold > 0 {
		toolOutputHandler.SetThreshold(ag.LargeOutputThreshold)
		logger.Info("Context offloading threshold set", loggerv2.Int("threshold", ag.LargeOutputThreshold))
	}

	// Large output handling is now done via virtual tools, not MCP server
	// Virtual tools are enabled by default and handle file operations directly
	toolOutputHandler.SetServerAvailable(true) // Always available with virtual tools

	// Set session ID for organizing files by conversation
	toolOutputHandler.SetSessionID(string(ag.TraceID))

	// Update the existing agent with connection data
	ag.Client = firstClient
	ag.Clients = clients
	ag.toolToServer = toolToServer
	ag.SystemPrompt = systemPrompt
	ag.servers = servers
	ag.toolOutputHandler = toolOutputHandler
	ag.prompts = prompts
	ag.resources = resources
	ag.configPath = configPath

	// Set selectedServers based on serverName parameter if not already set via options
	// This ensures discover_code_structure filters correctly when a single server is specified
	// IMPORTANT: Only auto-assign selectedServers if BOTH selectedServers AND selectedTools are empty
	// If selectedTools is set, the user wants specific tool filtering, not all tools from the server
	if len(ag.selectedServers) == 0 && len(ag.selectedTools) == 0 && serverName != "" && serverName != "all" {
		// serverName was specified and no filtering was configured via options
		// Use the servers list from NewAgentConnection (which already filtered based on serverName)
		ag.selectedServers = servers
		logger.Debug("Set selectedServers from serverName parameter",
			loggerv2.Any("selected_servers", ag.selectedServers))
	} else if len(ag.selectedServers) == 0 && len(ag.selectedTools) > 0 {
		// selectedTools is set but selectedServers is not - respect the specific tool filtering
		logger.Debug("Using selectedTools for filtering, not auto-assigning selectedServers",
			loggerv2.Any("selected_tools", ag.selectedTools))
	}

	// Create unified ToolFilter for consistent filtering across both modes
	// This filter is used by both LLM tool registration and discovery
	customCategories := ag.GetCustomToolCategories()
	ag.toolFilter = NewToolFilter(
		ag.selectedTools,
		ag.selectedServers,
		clients,
		customCategories,
		logger,
	)

	// Handle code execution mode: filter out MCP tools and custom tools if enabled
	var toolsToUse []llmtypes.Tool
	if ag.UseCodeExecutionMode {
		// Code execution mode: Only include virtual tools (discover_code_files, write_code)
		// Exclude all MCP server tools and custom tools (they'll be accessed via generated code)
		logger.Debug("Code execution mode enabled - excluding MCP tools and custom tools from LLM (will use generated code)")

		// Build set of custom tool names for filtering
		customToolNames := make(map[string]bool)
		for toolName := range ag.customTools {
			customToolNames[toolName] = true
		}

		for _, tool := range allLLMTools {
			// Check if this tool is an MCP tool (exists in toolToServer)
			_, isMCPTool := toolToServer[tool.Function.Name]
			// Check if this tool is a custom tool
			isCustomTool := customToolNames[tool.Function.Name]

			// In code execution mode, exclude both MCP tools and custom tools
			// Only include virtual tools (which will be filtered later to only discover_code_files and write_code)
			if !isMCPTool && !isCustomTool {
				// Not an MCP tool or custom tool - include it (virtual tools only)
				toolsToUse = append(toolsToUse, tool)
			}
		}
		logger.Debug("Code execution mode: tools available (only virtual tools, MCP and custom tools excluded)",
			loggerv2.Int("tool_count", len(toolsToUse)))
	} else {
		// Normal mode: Use all tools
		toolsToUse = allLLMTools
	}

	ag.Tools = toolsToUse
	ag.filteredTools = toolsToUse

	// Apply selected tools filter using unified ToolFilter
	// This ensures consistent filtering between LLM tools and discovery
	// Empty selectedTools/selectedServers means "use all tools" (no filtering)
	// Non-empty means "use only matching tools"
	// Also supports "server:*" pattern to explicitly request all tools from a server
	if !ag.toolFilter.IsNoFilteringActive() {
		logger.Debug("Tool filtering active",
			loggerv2.Int("selected_tools", len(ag.selectedTools)),
			loggerv2.Int("selected_servers", len(ag.selectedServers)))

		// Build set of custom tool names for category determination
		customToolNames := make(map[string]bool)
		for toolName, customTool := range ag.customTools {
			customToolNames[toolName] = true
			// Also store category for this tool
			if customTool.Category != "" {
				customToolNames[customTool.Category+"_tools:"+toolName] = true
			}
		}

		// Filter tools using unified ToolFilter
		var filteredTools []llmtypes.Tool
		for _, tool := range toolsToUse {
			if tool.Function == nil {
				continue
			}
			toolName := tool.Function.Name

			// Determine the package/server name and tool type
			serverName, isMCPTool := toolToServer[toolName]
			isCustomTool := customToolNames[toolName]

			// Determine package name for custom tools
			var packageName string
			if isMCPTool {
				packageName = serverName
			} else if isCustomTool {
				// Find the category for this custom tool
				if customTool, ok := ag.customTools[toolName]; ok && customTool.Category != "" {
					packageName = customTool.Category + "_tools"
				} else {
					packageName = "custom_tools"
				}
			} else {
				// Virtual tool - always include
				filteredTools = append(filteredTools, tool)
				continue
			}

			// Use unified filter to check if tool should be included
			// Virtual tools are handled above, so isVirtualTool=false here
			if ag.toolFilter.ShouldIncludeTool(packageName, toolName, isCustomTool, false) {
				filteredTools = append(filteredTools, tool)
			}
		}

		logger.Debug("Tool filtering complete",
			loggerv2.Int("selected_tools", len(filteredTools)),
			loggerv2.Int("total_tools", len(toolsToUse)))
		ag.Tools = filteredTools
		ag.filteredTools = filteredTools
	} else {
		// No filtering active - use all available tools (already filtered by code execution mode if enabled)
		logger.Debug("Using all available tools (no filtering applied)",
			loggerv2.Int("tool_count", len(toolsToUse)))
		ag.Tools = toolsToUse
		ag.filteredTools = toolsToUse
	}

	// Initialize tool registry for code execution
	// Convert custom tools to executor functions
	customToolExecutors := make(map[string]func(ctx context.Context, args map[string]interface{}) (string, error))
	for name, customTool := range ag.customTools {
		customToolExecutors[name] = customTool.Execution
	}

	// Add virtual tools to the LLM tools list
	virtualTools := ag.CreateVirtualTools()

	// Filter virtual tools based on code execution mode
	if ag.UseCodeExecutionMode {
		// In code execution mode, only include discover_code_files and write_code
		var filteredVirtualTools []llmtypes.Tool
		for _, tool := range virtualTools {
			if tool.Function != nil {
				toolName := tool.Function.Name
				// Only include code execution tools in code execution mode
				if toolName == "discover_code_files" || toolName == "write_code" {
					filteredVirtualTools = append(filteredVirtualTools, tool)
				}
			}
		}
		virtualTools = filteredVirtualTools
		logger.Debug("Code execution mode: Filtered virtual tools - only discover_code_files and write_code available")
	} else {
		// In non-code execution mode, exclude discover_code_files and write_code
		var filteredVirtualTools []llmtypes.Tool
		for _, tool := range virtualTools {
			if tool.Function != nil {
				toolName := tool.Function.Name
				// Exclude code execution tools in non-code execution mode
				if toolName != "discover_code_files" && toolName != "write_code" {
					filteredVirtualTools = append(filteredVirtualTools, tool)
				}
			}
		}
		virtualTools = filteredVirtualTools
		logger.Debug("Non-code execution mode: Excluded discover_code_files and write_code from virtual tools")
	}

	ag.Tools = append(ag.Tools, virtualTools...)

	// Convert virtual tools to executor functions
	// Note: We need to capture the tool name in the closure
	virtualToolExecutors := make(map[string]func(ctx context.Context, args map[string]interface{}) (string, error))
	for _, virtualTool := range virtualTools {
		if virtualTool.Function != nil {
			toolName := virtualTool.Function.Name
			// Create a closure that captures the tool name and agent reference
			virtualToolExecutors[toolName] = func(name string) func(ctx context.Context, args map[string]interface{}) (string, error) {
				return func(ctx context.Context, args map[string]interface{}) (string, error) {
					return ag.HandleVirtualTool(ctx, name, args)
				}
			}(toolName)
		}
	}

	// Initialize registry with virtual tools
	codeexec.InitRegistryWithVirtualTools(ag.Clients, customToolExecutors, virtualToolExecutors, ag.toolToServer, logger)

	// Generate Go code for virtual tools (only needed in code execution mode)
	// In simple agent mode, virtual tools are called directly via HandleVirtualTool()
	// The generated code is only used when LLM writes Go code that imports these packages
	var generatedDir string
	if ag.UseCodeExecutionMode {
		generatedDir = ag.getGeneratedDir()
		// Use agent's ToolTimeout (same as used for normal tool calls)
		toolTimeout := getToolExecutionTimeout(ag)
		if err := codegen.GenerateVirtualToolsCode(virtualTools, generatedDir, logger, toolTimeout); err != nil {
			logger.Warn("Failed to generate Go code for virtual tools", loggerv2.Error(err))
			// Don't fail agent initialization if code generation fails
		}
		logger.Debug("MCP server code generation handled by cache manager (no regeneration needed)")
	}

	// In code execution mode, discover tool structure and include it in system prompt
	var toolStructureJSON string
	if ag.UseCodeExecutionMode {
		// Discover all available tools and include structure in system prompt
		toolStructure, err := ag.discoverAllServersAndTools(generatedDir)
		if err != nil {
			logger.Warn("Failed to discover tool structure for system prompt", loggerv2.Error(err))
			// Continue without tool structure if discovery fails
		} else {
			toolStructureJSON = toolStructure
		}
	}

	// Always rebuild system prompt with the correct agent mode and tool structure
	// This ensures Simple agents get Simple prompts and ReAct agents get ReAct prompts
	// In code execution mode, tool structure is automatically included
	if !ag.hasCustomSystemPrompt {
		ag.SystemPrompt = prompt.BuildSystemPromptWithoutTools(ag.prompts, ag.resources, string(ag.AgentMode), ag.DiscoverResource, ag.DiscoverPrompt, ag.UseCodeExecutionMode, toolStructureJSON, ag.Logger)
	}

	// ðŸŽ¯ SMART ROUTING INITIALIZATION - Run AFTER all tools are loaded (including virtual tools)
	// This ensures we have the complete tool count for accurate smart routing decisions

	if ag.shouldUseSmartRouting() {
		// Get server count for logging
		serverCount := len(ag.Clients)
		serverType := "active"

		logger.Info("Smart routing enabled - determining relevant tools after full initialization")
		logger.Debug("Total tools loaded",
			loggerv2.Int("tool_count", len(ag.Tools)),
			loggerv2.String("server_type", serverType),
			loggerv2.Int("server_count", serverCount),
			loggerv2.Int("max_tools_threshold", ag.SmartRoutingThreshold.MaxTools),
			loggerv2.Int("max_servers_threshold", ag.SmartRoutingThreshold.MaxServers))

		// For now, use all tools since we don't have conversation context yet
		// Smart routing will be re-evaluated in AskWithHistory with full conversation context
		ag.filteredTools = ag.Tools
		logger.Debug("Smart routing will be applied during conversation with full context")
	} else {
		// Get server count for logging
		serverCount := len(ag.Clients)
		serverType := "active"
		logger.Debug("Active mode",
			loggerv2.Int("client_count", serverCount))

		// No smart routing - use all tools
		ag.filteredTools = ag.Tools
		logger.Debug("Smart routing disabled - using all tools",
			loggerv2.Int("tool_count", len(ag.Tools)),
			loggerv2.String("server_type", serverType),
			loggerv2.Int("server_count", serverCount),
			loggerv2.Int("max_tools_threshold", ag.SmartRoutingThreshold.MaxTools),
			loggerv2.Int("max_servers_threshold", ag.SmartRoutingThreshold.MaxServers))
	}

	// No more event listeners - events go directly to tracer
	// Langfuse tracing is handled by the tracer itself

	// Agent initialization complete

	return ag, nil
}

// SetCurrentQuery sets the current query for hierarchy tracking
func (a *Agent) SetCurrentQuery(query string) {
	// This method is no longer needed as hierarchy is removed
}

// StartAgentSession creates a new agent-level event tree
func (a *Agent) StartAgentSession(ctx context.Context) {
	// Emit agent start event to create hierarchy
	agentStartEvent := events.NewAgentStartEvent(string(a.AgentMode), a.ModelID, string(a.provider), a.UseCodeExecutionMode)
	a.EmitTypedEvent(ctx, agentStartEvent)
}

// StartTurn creates a new turn-level event tree
func (a *Agent) StartTurn(ctx context.Context, turn int) {
	// Emit conversation turn event (this is already being emitted in conversation.go)
	// This method is kept for consistency but the actual turn event is emitted in AskWithHistory
}

// StartLLMGeneration creates a new LLM-level event tree
func (a *Agent) StartLLMGeneration(ctx context.Context) {
	// Emit LLM generation start event to create hierarchy
	llmStartEvent := events.NewLLMGenerationStartEvent(0, a.ModelID, a.Temperature, len(a.filteredTools), 0)
	a.EmitTypedEvent(ctx, llmStartEvent)
}

// extractCacheTokens extracts all cache-related tokens from GenerationInfo
// Supports multiple providers: OpenAI (CachedContentTokens), Anthropic (CacheReadInputTokens, CacheCreationInputTokens)
func extractCacheTokens(generationInfo *llmtypes.GenerationInfo) int {
	if generationInfo == nil {
		return 0
	}

	totalCacheTokens := 0

	// Check CachedContentTokens (OpenAI, Gemini)
	if generationInfo.CachedContentTokens != nil {
		totalCacheTokens += *generationInfo.CachedContentTokens
	}

	// Check Additional map for Anthropic cache tokens
	if generationInfo.Additional != nil {
		// CacheReadInputTokens (tokens read from cache)
		if cacheRead, ok := generationInfo.Additional["CacheReadInputTokens"]; ok {
			if cacheReadInt, ok := cacheRead.(int); ok {
				totalCacheTokens += cacheReadInt
			} else if cacheReadFloat, ok := cacheRead.(float64); ok {
				totalCacheTokens += int(cacheReadFloat)
			}
		}
		// Also check lowercase variant
		if cacheRead, ok := generationInfo.Additional["cache_read_input_tokens"]; ok {
			if cacheReadInt, ok := cacheRead.(int); ok {
				totalCacheTokens += cacheReadInt
			} else if cacheReadFloat, ok := cacheRead.(float64); ok {
				totalCacheTokens += int(cacheReadFloat)
			}
		}

		// CacheCreationInputTokens (tokens used to create cache)
		if cacheCreate, ok := generationInfo.Additional["CacheCreationInputTokens"]; ok {
			if cacheCreateInt, ok := cacheCreate.(int); ok {
				totalCacheTokens += cacheCreateInt
			} else if cacheCreateFloat, ok := cacheCreate.(float64); ok {
				totalCacheTokens += int(cacheCreateFloat)
			}
		}
		// Also check lowercase variant
		if cacheCreate, ok := generationInfo.Additional["cache_creation_input_tokens"]; ok {
			if cacheCreateInt, ok := cacheCreate.(int); ok {
				totalCacheTokens += cacheCreateInt
			} else if cacheCreateFloat, ok := cacheCreate.(float64); ok {
				totalCacheTokens += int(cacheCreateFloat)
			}
		}
	}

	return totalCacheTokens
}

// accumulateTokenUsage accumulates token usage from an LLM call.
// It accepts ContentResponse to use the unified Usage field, with fallback to GenerationInfo.
func (a *Agent) accumulateTokenUsage(ctx context.Context, usageMetrics events.UsageMetrics, resp *llmtypes.ContentResponse, turn int) {
	a.tokenTrackingMutex.Lock()
	defer a.tokenTrackingMutex.Unlock()

	var cacheTokens int
	var reasoningTokens int
	var cacheDiscount float64
	var generationInfo *llmtypes.GenerationInfo

	// Priority 1: Extract from unified Usage field (if available)
	if resp != nil && resp.Usage != nil {
		// Extract cache tokens from unified Usage
		if resp.Usage.CacheTokens != nil {
			cacheTokens = *resp.Usage.CacheTokens
		}

		// Extract reasoning tokens from unified Usage
		if resp.Usage.ReasoningTokens != nil {
			reasoningTokens = *resp.Usage.ReasoningTokens
		}
	}

	// Priority 2: Fall back to GenerationInfo (for cache discount and detailed breakdown)
	if resp != nil && len(resp.Choices) > 0 {
		generationInfo = resp.Choices[0].GenerationInfo
	}

	// Extract cache tokens from GenerationInfo if not found in Usage
	if cacheTokens == 0 && generationInfo != nil {
		cacheTokens = extractCacheTokens(generationInfo)
	}

	// Extract reasoning tokens from GenerationInfo if not found in Usage
	if reasoningTokens == 0 && generationInfo != nil && generationInfo.ReasoningTokens != nil {
		reasoningTokens = *generationInfo.ReasoningTokens
	}

	// Extract cache discount (only available in GenerationInfo)
	if generationInfo != nil && generationInfo.CacheDiscount != nil {
		cacheDiscount = *generationInfo.CacheDiscount
	}

	// Accumulate tokens
	a.cumulativePromptTokens += usageMetrics.PromptTokens
	a.cumulativeCompletionTokens += usageMetrics.CompletionTokens
	a.cumulativeTotalTokens += usageMetrics.TotalTokens
	a.cumulativeCacheTokens += cacheTokens
	a.cumulativeReasoningTokens += reasoningTokens
	a.cumulativeCacheDiscount += cacheDiscount
	a.llmCallCount++

	if cacheTokens > 0 {
		a.cacheEnabledCallCount++
	}

	// Token usage is tracked via events - log at debug level for per-turn, but also log cumulative
	logger := getLogger(a)
	logger.Debug("Turn tokens",
		loggerv2.Int("turn", turn),
		loggerv2.Int("input_tokens", usageMetrics.PromptTokens),
		loggerv2.Int("output_tokens", usageMetrics.CompletionTokens),
		loggerv2.Int("total_tokens", usageMetrics.TotalTokens),
		loggerv2.Int("cache_tokens", cacheTokens),
		loggerv2.Int("reasoning_tokens", reasoningTokens),
		loggerv2.Int("cumulative_total", a.cumulativeTotalTokens))
}

// EndLLMGeneration ends the current LLM generation
func (a *Agent) EndLLMGeneration(ctx context.Context, result string, turn int, toolCalls int, duration time.Duration, usageMetrics events.UsageMetrics, resp *llmtypes.ContentResponse) {
	// Accumulate token usage (including cache tokens) - uses unified Usage field
	a.accumulateTokenUsage(ctx, usageMetrics, resp, turn)

	// Extract cache and reasoning tokens to include in UsageMetrics
	// Priority: Use unified Usage field, fall back to GenerationInfo
	var cacheTokens int
	var reasoningTokens int

	if resp != nil && resp.Usage != nil {
		if resp.Usage.CacheTokens != nil {
			cacheTokens = *resp.Usage.CacheTokens
		}
		if resp.Usage.ReasoningTokens != nil {
			reasoningTokens = *resp.Usage.ReasoningTokens
		}
	}

	// Fall back to GenerationInfo if not found in Usage
	if (cacheTokens == 0 || reasoningTokens == 0) && resp != nil && len(resp.Choices) > 0 && resp.Choices[0].GenerationInfo != nil {
		generationInfo := resp.Choices[0].GenerationInfo
		if cacheTokens == 0 {
			cacheTokens = extractCacheTokens(generationInfo)
		}
		if reasoningTokens == 0 && generationInfo.ReasoningTokens != nil {
			reasoningTokens = *generationInfo.ReasoningTokens
		}
	}

	// Add cache and reasoning tokens to usage metrics
	usageMetrics.CacheTokens = cacheTokens
	usageMetrics.ReasoningTokens = reasoningTokens

	// Emit LLM generation end event with complete token information
	llmEndEvent := events.NewLLMGenerationEndEvent(turn, result, toolCalls, duration, usageMetrics)
	a.EmitTypedEvent(ctx, llmEndEvent)
}

// EndTurn ends the current turn
func (a *Agent) EndTurn(ctx context.Context) {
	// This method is no longer needed as hierarchy is removed
}

// emitTotalTokenUsageEvent emits a total token usage event with all cumulative metrics
func (a *Agent) emitTotalTokenUsageEvent(ctx context.Context, conversationDuration time.Duration) {
	a.tokenTrackingMutex.RLock()
	defer a.tokenTrackingMutex.RUnlock()

	// Create generation info map with cumulative cache information
	generationInfo := make(map[string]interface{})
	generationInfo["cumulative_prompt_tokens"] = a.cumulativePromptTokens
	generationInfo["cumulative_completion_tokens"] = a.cumulativeCompletionTokens
	generationInfo["cumulative_total_tokens"] = a.cumulativeTotalTokens
	generationInfo["cumulative_cache_tokens"] = a.cumulativeCacheTokens
	generationInfo["cumulative_reasoning_tokens"] = a.cumulativeReasoningTokens
	generationInfo["llm_call_count"] = a.llmCallCount
	generationInfo["cache_enabled_call_count"] = a.cacheEnabledCallCount

	// Emit total token usage event
	totalTokenEvent := events.NewTokenUsageEventWithCache(
		0, // turn (this is a summary event, not tied to a specific turn)
		"conversation_total",
		a.ModelID,
		string(a.provider),
		a.cumulativePromptTokens,
		a.cumulativeCompletionTokens,
		a.cumulativeTotalTokens,
		conversationDuration,
		"conversation_total",
		0.0, // cache discount removed
		a.cumulativeReasoningTokens,
		generationInfo,
	)

	a.EmitTypedEvent(ctx, totalTokenEvent)

	// Log total token usage summary at Info level for visibility
	logger := getLogger(a)
	logger.Info("ðŸ”§ [TOKEN_USAGE] Conversation total token usage",
		loggerv2.Int("total_tokens", a.cumulativeTotalTokens),
		loggerv2.Int("input_tokens", a.cumulativePromptTokens),
		loggerv2.Int("output_tokens", a.cumulativeCompletionTokens),
		loggerv2.Int("cache_tokens", a.cumulativeCacheTokens),
		loggerv2.Int("reasoning_tokens", a.cumulativeReasoningTokens),
		loggerv2.Int("llm_calls", a.llmCallCount),
		loggerv2.Int("cache_enabled_calls", a.cacheEnabledCallCount),
		loggerv2.Any("duration", conversationDuration))
	logger.Info("============================================================")
}

// GetTokenUsage returns the current cumulative token usage metrics
func (a *Agent) GetTokenUsage() (promptTokens, completionTokens, totalTokens, cacheTokens, reasoningTokens, llmCallCount, cacheEnabledCallCount int) {
	a.tokenTrackingMutex.RLock()
	defer a.tokenTrackingMutex.RUnlock()

	promptTokens = a.cumulativePromptTokens
	completionTokens = a.cumulativeCompletionTokens
	totalTokens = a.cumulativeTotalTokens
	cacheTokens = a.cumulativeCacheTokens
	reasoningTokens = a.cumulativeReasoningTokens
	llmCallCount = a.llmCallCount
	cacheEnabledCallCount = a.cacheEnabledCallCount
	return
}

// EndAgentSession ends the current agent session
func (a *Agent) EndAgentSession(ctx context.Context, conversationDuration time.Duration) {
	// Emit total token usage event before agent end event
	a.emitTotalTokenUsageEvent(ctx, conversationDuration)

	// Read cumulative token metrics for agent_end event
	promptTokens, completionTokens, totalTokens, cacheTokens, reasoningTokens, llmCallCount, cacheEnabledCallCount := a.GetTokenUsage()

	// Emit agent end event with token usage information
	agentEndEvent := events.NewAgentEndEventWithTokens(
		string(a.AgentMode),
		true,
		"",
		promptTokens,
		completionTokens,
		totalTokens,
		cacheTokens,
		reasoningTokens,
		llmCallCount,
		cacheEnabledCallCount,
	)
	a.EmitTypedEvent(ctx, agentEndEvent)

	// Cleanup agent-specific generated directory (only in code execution mode)
	if a.UseCodeExecutionMode {
		a.cleanupAgentGeneratedDir()
	}
}

// cleanupAgentGeneratedDir removes the agent-specific generated directory
func (a *Agent) cleanupAgentGeneratedDir() {
	agentDir := a.getAgentGeneratedDir()

	// Check if directory exists
	if _, err := os.Stat(agentDir); os.IsNotExist(err) {
		// Directory doesn't exist, nothing to clean
		return
	}

	// Remove the entire agent directory
	if err := os.RemoveAll(agentDir); err != nil {
		if a.Logger != nil {
			a.Logger.Warn("âš ï¸ Failed to cleanup agent directory", loggerv2.Error(err), loggerv2.String("directory", agentDir))
		}
	} else if a.Logger != nil {
		a.Logger.Info("ðŸ§¹ Cleaned up agent directory", loggerv2.String("directory", agentDir))
	}
}

// RebuildSystemPromptWithFilteredServers rebuilds the system prompt with only prompts/resources from relevant servers
func (a *Agent) RebuildSystemPromptWithFilteredServers(ctx context.Context, relevantServers []string) error {
	logger := a.Logger
	logger.Info("ðŸ”„ Rebuilding system prompt with filtered servers",
		loggerv2.Any("relevant_servers", relevantServers),
		loggerv2.Int("total_servers", len(a.Clients)))

	// Get fresh prompts and resources from unified cache using simple server names
	filteredPrompts := make(map[string][]mcp.Prompt)
	filteredResources := make(map[string][]mcp.Resource)

	// Load MCP configuration to get server configs for cache keys
	config, err := mcpclient.LoadMergedConfig(a.configPath, logger)
	if err != nil {
		logger.Warn("Failed to load MCP config for cache lookup", loggerv2.Error(err))
		return fmt.Errorf("failed to load MCP config: %w", err)
	}

	// Get cache manager
	cacheManager := mcpcache.GetCacheManager(logger)

	for _, serverName := range relevantServers {
		// Get server configuration for this server
		serverConfig, exists := config.MCPServers[serverName]
		if !exists {
			logger.Warn("Server configuration not found, skipping cache lookup", loggerv2.String("server", serverName))
			continue
		}

		// Generate configuration-aware cache key
		cacheKey := mcpcache.GenerateUnifiedCacheKey(serverName, serverConfig)

		// Try to get cached data
		cachedEntry, found := cacheManager.Get(cacheKey)
		if !found {
			logger.Debug("Cache miss for server", loggerv2.String("server", serverName))
			continue
		}

		if cachedEntry != nil && cachedEntry.IsValid {
			logger.Info("âœ… Cache hit for server - using cached prompts and resources", loggerv2.String("server", serverName))

			// Add cached prompts and resources to filtered collections
			if len(cachedEntry.Prompts) > 0 {
				filteredPrompts[serverName] = cachedEntry.Prompts
			}
			if len(cachedEntry.Resources) > 0 {
				filteredResources[serverName] = cachedEntry.Resources
			}
		} else {
			logger.Debug("Cache miss or invalid entry for server", loggerv2.String("server", serverName))
		}
	}

	// Rebuild system prompt with filtered data
	// In code execution mode, rediscover tool structure after filtering
	var toolStructureJSON string
	if a.UseCodeExecutionMode {
		generatedDir := a.getGeneratedDir()
		toolStructure, err := a.discoverAllServersAndTools(generatedDir)
		if err != nil {
			if a.Logger != nil {
				a.Logger.Warn("Failed to rediscover tool structure after filtering", loggerv2.Error(err))
			}
		} else {
			toolStructureJSON = toolStructure
		}
	}
	newSystemPrompt := prompt.BuildSystemPromptWithoutTools(
		filteredPrompts,
		filteredResources,
		string(a.AgentMode),
		a.DiscoverResource,
		a.DiscoverPrompt,
		a.UseCodeExecutionMode,
		toolStructureJSON,
		a.Logger,
	)

	// Update the agent's system prompt
	a.SystemPrompt = newSystemPrompt

	logger.Info("âœ… System prompt rebuilt with filtered servers",
		loggerv2.Int("filtered_prompts_count", len(filteredPrompts)),
		loggerv2.Int("filtered_resources_count", len(filteredResources)),
		loggerv2.Int("new_prompt_length", len(newSystemPrompt)))

	return nil
}

// NewAgentWithObservability creates a new Agent with observability configuration
// The modelID is automatically extracted from the LLM instance
// This function automatically sets up a noop tracer and generates a trace ID if not provided via options
// By default, connects to all servers (AllServers). Use WithServerName() to filter.
func NewAgentWithObservability(ctx context.Context, llm llmtypes.Model, configPath string, options ...AgentOption) (*Agent, error) {
	if llm == nil {
		return nil, fmt.Errorf("LLM cannot be nil")
	}

	// Extract model ID from LLM instance
	modelID := extractModelIDFromLLM(llm)

	// Create agent with default values first to apply options
	ag := &Agent{
		ctx:                           ctx,
		LLM:                           llm,
		Tracers:                       []observability.Tracer{}, // Default: empty tracers array
		MaxTurns:                      GetDefaultMaxTurns(SimpleAgent),
		Temperature:                   0.0,
		ToolChoice:                    "auto",
		ModelID:                       modelID,
		AgentMode:                     SimpleAgent,
		TraceID:                       "", // Will be generated if not set via options
		EnableLargeOutputVirtualTools: true,
		Logger:                        loggerv2.NewDefault(), // Default logger
		customTools:                   make(map[string]CustomTool),
		EnableSmartRouting:            false,
		DiscoverResource:              true,
		DiscoverPrompt:                true,
		DisableCache:                  false,                // Default: cache enabled
		serverName:                    mcpclient.AllServers, // Default: all servers
	}

	// Apply all options
	for _, option := range options {
		option(ag)
	}

	// Use logger from options (or default if not set)
	logger := ag.Logger
	if logger == nil {
		logger = loggerv2.NewDefault()
		ag.Logger = logger
	}

	// Use serverName from options (or default AllServers)
	serverName := ag.serverName
	if serverName == "" {
		serverName = mcpclient.AllServers
	}

	if modelID == "unknown" {
		logger.Warn("Could not extract model ID from LLM instance, using 'unknown'",
			loggerv2.String("fallback", "unknown"))
	}

	// If no tracer was provided via options, create a noop tracer
	if len(ag.Tracers) == 0 {
		baseTracer := observability.GetTracerWithLogger("noop", logger)
		streamingTracer := NewStreamingTracer(baseTracer, 100)
		ag.Tracers = []observability.Tracer{streamingTracer}
	}

	// If no trace ID was provided via options, generate one
	if ag.TraceID == "" {
		ag.TraceID = observability.TraceID(fmt.Sprintf("agent-session-%s-%d", modelID, time.Now().UnixNano()))
	}

	clients, toolToServer, allLLMTools, servers, prompts, resources, systemPrompt, err := NewAgentConnection(ctx, llm, serverName, configPath, string(ag.TraceID), ag.Tracers, logger, ag.DisableCache)
	if err != nil {
		return nil, err
	}

	// Use first client for legacy compatibility
	var firstClient mcpclient.ClientInterface
	if len(clients) > 0 {
		for _, c := range clients {
			firstClient = c
			break
		}
	}

	// Initialize tool output handler for context offloading
	toolOutputHandler := NewToolOutputHandler()

	// Apply custom threshold if set via WithLargeOutputThreshold option
	if ag.LargeOutputThreshold > 0 {
		toolOutputHandler.SetThreshold(ag.LargeOutputThreshold)
		logger.Info("Context offloading threshold set", loggerv2.Int("threshold", ag.LargeOutputThreshold))
	}

	// Context offloading is done via virtual tools, not MCP server
	// Virtual tools are enabled by default and handle file operations directly
	toolOutputHandler.SetServerAvailable(true) // Always available with virtual tools

	// Set session ID for organizing files by conversation
	toolOutputHandler.SetSessionID(string(ag.TraceID))

	// Debug logging for context offloading (observability version)
	// Use the logger we created earlier
	logger.Info("ðŸ” Context offloading via virtual tools (observability)",
		loggerv2.Any("virtual_tools_enabled", true),
		loggerv2.Int("total_clients", len(clients)),
		loggerv2.Any("client_names", getClientNames(clients)))

	// Update the agent struct with connection data
	ag.Client = firstClient
	ag.Clients = clients
	ag.toolToServer = toolToServer
	ag.LLM = llm
	ag.Tools = allLLMTools
	ag.SystemPrompt = systemPrompt
	ag.servers = servers
	ag.toolOutputHandler = toolOutputHandler
	ag.prompts = prompts
	ag.resources = resources

	// No more event listeners - events go directly to tracer
	// Tracing is handled by the tracer itself based on TRACING_PROVIDER

	// Agent initialization complete

	return ag, nil
}

// Convenience constructors for common use cases
// By default, connects to all servers (AllServers). Use WithServerName() to filter.
func NewSimpleAgent(ctx context.Context, llm llmtypes.Model, configPath string, options ...AgentOption) (*Agent, error) {
	return NewAgent(ctx, llm, configPath, append(options, WithMode(SimpleAgent))...)
}

// Legacy constructors have been removed to enforce proper logger usage
// Use NewAgent or NewSimpleAgent with functional options instead

// AddEventListener and EmitEvent methods have been removed - events now go directly to tracers

// AddEventListener adds an event listener to the agent
func (a *Agent) AddEventListener(listener AgentEventListener) {
	a.mu.Lock()
	defer a.mu.Unlock()

	if a.listeners == nil {
		a.listeners = make([]AgentEventListener, 0)
	}
	a.listeners = append(a.listeners, listener)

	// ðŸ†• NEW: Enable streaming tracer when event listeners are added
	// This provides streaming capabilities to external systems
	if _, hasStreaming := a.GetStreamingTracer(); hasStreaming {
		a.Logger.Info("ðŸ” Streaming tracer enabled for event listener", loggerv2.String("listener", listener.Name()))

		// The streaming tracer is already active and will forward events to all listeners
		// No additional setup needed - events automatically flow through the streaming system
	} else {
		a.Logger.Warn("Streaming tracer not available, using traditional event listener system")
	}
}

// RemoveEventListener removes an event listener from the agent
func (a *Agent) RemoveEventListener(listener AgentEventListener) {
	a.mu.Lock()
	defer a.mu.Unlock()

	for i, l := range a.listeners {
		if l == listener {
			a.listeners = append(a.listeners[:i], a.listeners[i+1:]...)
			break
		}
	}
}

// initializeHierarchyForContext sets the initial hierarchy level based on calling context
func (a *Agent) initializeHierarchyForContext(ctx context.Context) {
	// âœ… SIMPLIFIED APPROACH: Detect context by checking stack trace or other indicators

	// Check if we're in orchestrator context by looking for orchestrator-related context values
	if orchestratorID := ctx.Value("orchestrator_id"); orchestratorID != nil {
		// Orchestrator context: Start at level 2 (orchestrator_start -> orchestrator_agent_start -> system_prompt)
		a.currentHierarchyLevel = 2
		a.currentParentEventID = fmt.Sprintf("orchestrator_agent_start_%d", time.Now().UnixNano())
		return
	}

	// Check if we're in server context (HTTP API call) by looking for session-related context values
	if sessionID := ctx.Value("session_id"); sessionID != nil {
		// Server context: Start at level 0 (system_prompt is root)
		a.currentHierarchyLevel = 0
		a.currentParentEventID = ""
		return
	}

	// âœ… FALLBACK: Always start at level 0 for now
	// This ensures consistent behavior until we implement proper context detection
	a.currentHierarchyLevel = 0
	a.currentParentEventID = ""
}

// EmitTypedEvent sends a typed event to all tracers AND all listeners
func (a *Agent) EmitTypedEvent(ctx context.Context, eventData events.EventData) {

	// âœ… SET HIERARCHY FIELDS ON EVENT DATA FIRST (SINGLE SOURCE OF TRUTH)
	// Use interface-based approach - works for ALL event types that embed BaseEventData
	if baseEventData, ok := eventData.(interface {
		SetHierarchyFields(string, int, string, string)
	}); ok {
		baseEventData.SetHierarchyFields(a.currentParentEventID, a.currentHierarchyLevel, string(a.TraceID), events.GetComponentFromEventType(eventData.GetEventType()))
	}

	// Create event with correlation ID for start/end event pairs
	event := events.NewAgentEvent(eventData)
	event.TraceID = string(a.TraceID)

	// Generate a unique SpanID for this event
	event.SpanID = fmt.Sprintf("span_%s_%d", string(eventData.GetEventType()), time.Now().UnixNano())

	// âœ… COPY HIERARCHY FIELDS FROM EVENT DATA TO WRAPPER (SINGLE SOURCE OF TRUTH)
	// Get hierarchy fields from the event data (which we just set above)
	// Use interface to access BaseEventData fields from any event type
	if baseEventData, ok := eventData.(interface{ GetBaseEventData() *events.BaseEventData }); ok {
		baseData := baseEventData.GetBaseEventData()
		event.ParentID = baseData.ParentID
		event.HierarchyLevel = baseData.HierarchyLevel
		event.SessionID = baseData.SessionID
		event.Component = baseData.Component
	}

	// Update hierarchy for next event based on event type
	eventType := events.EventType(eventData.GetEventType())

	if events.IsStartEvent(eventType) {
		// âœ… SPECIAL HANDLING: conversation_turn should reset to level 2 (child of conversation_start)
		switch eventType {
		case events.ConversationTurn:
			a.currentHierarchyLevel = 2 // Reset to level 2 for new conversation turn
			a.currentParentEventID = event.SpanID
		case events.ToolCallStart:
			// âœ… SPECIAL HANDLING: tool_call_start should be sibling of llm_generation_end
			// Don't increment level - use current level (same as llm_generation_end)
			a.currentParentEventID = event.SpanID
		default:
			// âœ… FIX: Increment level FIRST, then use it for next event
			a.currentHierarchyLevel++
			a.currentParentEventID = event.SpanID
		}
		// âœ… For end events: Level remains unchanged
		// SPECIAL HANDLING: tool_call_end should be sibling of tool_call_start
		// FIX: Don't decrement level immediately - let the next start event handle it
		// This allows token_usage and tool_call_start to be siblings of llm_generation_end
	}

	// Add correlation ID for start/end event pairs
	if isStartOrEndEvent(events.EventType(eventData.GetEventType())) {
		event.CorrelationID = fmt.Sprintf("%s_%d", string(eventData.GetEventType()), time.Now().UnixNano())
	}

	// Send to all tracers (multiple tracer support)
	// The streaming tracer will automatically forward events to subscribers
	for _, tracer := range a.Tracers {
		if err := tracer.EmitEvent(event); err != nil {
			a.Logger.Warn("Failed to emit event to tracer", loggerv2.Error(err), loggerv2.String("tracer_type", fmt.Sprintf("%T", tracer)))
		}
	}

	// ALSO send to all event listeners for backward compatibility
	// This ensures existing code continues to work while streaming is available
	a.mu.RLock()
	listeners := make([]AgentEventListener, len(a.listeners))
	copy(listeners, a.listeners)
	a.mu.RUnlock()

	for _, listener := range listeners {
		if err := listener.HandleEvent(ctx, event); err != nil {
			a.Logger.Warn("Failed to emit event to listener", loggerv2.Error(err), loggerv2.String("listener_type", fmt.Sprintf("%T", listener)))
		}
	}
}

// HandleEvent implements the WorkspaceEventEmitter interface for workspace tools.
// This allows workspace tools to emit events when called via the agent conversation loop.
// The workspace_tools.go file expects this interface to emit workspace_file_operation events.
func (a *Agent) HandleEvent(ctx context.Context, event *events.AgentEvent) error {
	if event != nil && event.Data != nil {
		a.EmitTypedEvent(ctx, event.Data)
	}
	return nil
}

// isStartOrEndEvent checks if an event type is a start or end event that needs correlation ID
func isStartOrEndEvent(eventType events.EventType) bool {
	return eventType == events.ConversationStart || eventType == events.ConversationEnd ||
		eventType == events.LLMGenerationStart || eventType == events.LLMGenerationEnd ||
		eventType == events.ToolCallStart || eventType == events.ToolCallEnd
}

// GetPrimaryTracer returns the first tracer for backward compatibility
func (a *Agent) GetPrimaryTracer() observability.Tracer {
	if len(a.Tracers) > 0 {
		return a.Tracers[0]
	}
	return observability.NoopTracer{}
}

// GetStreamingTracer returns the streaming tracer if available
func (a *Agent) GetStreamingTracer() (StreamingTracer, bool) {
	if len(a.Tracers) > 0 {
		if streamingTracer, ok := a.Tracers[0].(StreamingTracer); ok {
			return streamingTracer, true
		}
	}
	return nil, false
}

// HasStreamingCapability returns true if the agent supports event streaming
func (a *Agent) HasStreamingCapability() bool {
	_, hasStreaming := a.GetStreamingTracer()
	return hasStreaming
}

// GetEventStream returns the event stream channel if streaming is available
func (a *Agent) GetEventStream() (<-chan *events.AgentEvent, bool) {
	if streamingTracer, hasStreaming := a.GetStreamingTracer(); hasStreaming {
		return streamingTracer.GetEventStream(), true
	}
	return nil, false
}

// SubscribeToEvents allows external systems to subscribe to agent events
func (a *Agent) SubscribeToEvents(ctx context.Context) (<-chan *events.AgentEvent, func(), bool) {
	if streamingTracer, hasStreaming := a.GetStreamingTracer(); hasStreaming {
		eventChan, unsubscribe := streamingTracer.SubscribeToEvents(ctx)
		return eventChan, unsubscribe, true
	}
	return nil, func() {}, false
}

// getClientNames returns a list of client names for debugging
func getClientNames(clients map[string]mcpclient.ClientInterface) []string {
	names := make([]string, 0, len(clients))
	for name := range clients {
		names = append(names, name)
	}
	return names
}

// Close closes all underlying MCP client connections.
func (a *Agent) Close() {
	// Close all clients in the map
	for serverName, client := range a.Clients {
		if client != nil {
			a.Logger.Info(fmt.Sprintf("ðŸ”Œ Closing connection to %s", serverName), loggerv2.String("server_name", serverName))
			_ = client.Close() // Ignore errors during cleanup
		}
	}

	// Legacy single client cleanup (may be redundant but safe)
	if a.Client != nil {
		_ = a.Client.Close() // Ignore errors during cleanup
	}
}

// CheckConnectionHealth performs health checks on all MCP connections
func (a *Agent) CheckConnectionHealth(ctx context.Context) map[string]error {
	healthResults := make(map[string]error)

	for serverName, client := range a.Clients {
		if client == nil {
			healthResults[serverName] = fmt.Errorf("client is nil")
			continue
		}

		// Check if connection is active by trying to list tools
		_, err := client.ListTools(ctx)
		if err != nil {
			healthResults[serverName] = fmt.Errorf("connection health check failed: %w", err)
		}
	}

	return healthResults
}

// GetConnectionStats returns statistics about all MCP connections
func (a *Agent) GetConnectionStats() map[string]interface{} {
	stats := make(map[string]interface{})

	totalConnections := 0
	healthyConnections := 0
	activeServers := make([]string, 0)

	for serverName, client := range a.Clients {
		if client != nil {
			totalConnections++
			// Check if connection is healthy by trying to list tools
			_, err := client.ListTools(context.Background())
			if err == nil {
				healthyConnections++
				activeServers = append(activeServers, serverName)
			}
		}
	}

	stats["total_connections"] = totalConnections
	stats["healthy_connections"] = healthyConnections
	stats["active_servers"] = activeServers
	if totalConnections > 0 {
		stats["health_ratio"] = float64(healthyConnections) / float64(totalConnections)
	} else {
		stats["health_ratio"] = 0.0
	}

	return stats
}

// Ask runs a single-question interaction with possible tool calls and returns the final answer.
// Delegates to AskWithHistory with a single message
func (a *Agent) Ask(ctx context.Context, question string) (string, error) {
	// Create a single user message for the question
	userMessage := llmtypes.MessageContent{
		Role:  llmtypes.ChatMessageTypeHuman,
		Parts: []llmtypes.ContentPart{llmtypes.TextContent{Text: question}},
	}

	// Call AskWithHistory with the single message
	answer, _, err := AskWithHistory(a, ctx, []llmtypes.MessageContent{userMessage})
	return answer, err
}

// AskWithHistory runs an interaction using the provided message history (multi-turn conversation).
// Delegates to conversation.go
func (a *Agent) AskWithHistory(ctx context.Context, messages []llmtypes.MessageContent) (string, []llmtypes.MessageContent, error) {
	return AskWithHistory(a, ctx, messages)
}

// AskStructured runs a single-question interaction and converts the result to structured output
func AskStructured[T any](a *Agent, ctx context.Context, question string, schema T, schemaString string) (T, error) {
	// Create a single user message for the question
	userMessage := llmtypes.MessageContent{
		Role:  llmtypes.ChatMessageTypeHuman,
		Parts: []llmtypes.ContentPart{llmtypes.TextContent{Text: question}},
	}

	// Call AskWithHistoryStructured with the single message
	answer, _, err := AskWithHistoryStructured(a, ctx, []llmtypes.MessageContent{userMessage}, schema, schemaString)
	return answer, err
}

// AskWithHistoryStructured runs an interaction using message history and converts the result to structured output
func AskWithHistoryStructured[T any](a *Agent, ctx context.Context, messages []llmtypes.MessageContent, schema T, schemaString string) (T, []llmtypes.MessageContent, error) {
	// First, get the text response using the existing method
	textResponse, updatedMessages, err := a.AskWithHistory(ctx, messages)
	if err != nil {
		var zero T
		return zero, updatedMessages, fmt.Errorf("failed to get text response: %w", err)
	}

	// Convert the text response to structured output
	structuredResult, err := ConvertToStructuredOutput(a, ctx, textResponse, schema, schemaString)
	if err != nil {
		var zero T
		return zero, updatedMessages, fmt.Errorf("failed to convert to structured output: %w", err)
	}

	return structuredResult, updatedMessages, nil
}

// AskWithHistoryStructuredViaTool runs an interaction using message history and extracts structured output
// from a dynamically registered tool call. The LLM can call the tool during conversation, and we extract
// the structured data from the tool call arguments after conversation completes.
func AskWithHistoryStructuredViaTool[T any](
	a *Agent,
	ctx context.Context,
	messages []llmtypes.MessageContent,
	toolName string,
	toolDescription string,
	schema string,
) (StructuredOutputResult[T], error) {
	// Parse schema string to get tool parameters
	toolParams, err := parseSchemaForToolParameters(schema)
	if err != nil {
		var zero StructuredOutputResult[T]
		return zero, fmt.Errorf("failed to parse schema for tool parameters: %w", err)
	}

	// Create a cancellable context to break conversation as soon as tool is called
	toolCalledCtx, cancelToolCalled := context.WithCancel(ctx)
	defer cancelToolCalled()

	// Channel to signal that tool was called (thread-safe)
	toolCalledChan := make(chan bool, 1)

	// Register custom tool dynamically
	// The execution function signals that tool was called and cancels the context to break immediately
	executionFunc := func(ctx context.Context, args map[string]interface{}) (string, error) {
		// Signal that tool was called (non-blocking)
		select {
		case toolCalledChan <- true:
		default:
		}
		// Cancel the context to break the conversation immediately
		cancelToolCalled()
		// Return minimal message - we'll break immediately so this won't be processed
		return "", nil
	}

	// Register with "structured_output" category so it's always available even in code execution mode
	if err := a.RegisterCustomTool(toolName, toolDescription, toolParams, executionFunc, "structured_output"); err != nil {
		var zero StructuredOutputResult[T]
		return zero, fmt.Errorf("failed to register custom tool: %w", err)
	}

	// Call existing AskWithHistory - will break as soon as tool is called
	textResponse, updatedMessages, err := a.AskWithHistory(toolCalledCtx, messages)

	// Check if tool was called (non-blocking check)
	toolCalled := false
	select {
	case <-toolCalledChan:
		toolCalled = true
	default:
	}

	// If tool was called, context cancellation is expected - we still need to extract structured output
	if toolCalled {
		// Scan messages for structured tool call (even if AskWithHistory returned error due to cancellation)
		structuredResult, found, extractErr := extractStructuredToolCall[T](updatedMessages, toolName)
		if extractErr != nil {
			var zero StructuredOutputResult[T]
			return zero, fmt.Errorf("tool was called but structured output extraction failed: %w", extractErr)
		}

		if found {
			// Structured tool was called - return structured result immediately
			return StructuredOutputResult[T]{
				HasStructuredOutput: true,
				StructuredResult:    structuredResult,
				TextResponse:        "",
				Messages:            updatedMessages,
			}, nil
		}

		// Tool was called but not found in messages - error
		var zero StructuredOutputResult[T]
		return zero, fmt.Errorf("tool was called but not found in messages")
	}

	// Tool was not called according to flag - but check messages anyway
	// (context cancellation might have happened even if tool was called)
	// Scan messages for structured tool call (in case it was called but flag wasn't set)
	structuredResult, found, extractErr := extractStructuredToolCall[T](updatedMessages, toolName)
	if extractErr != nil {
		var zero StructuredOutputResult[T]
		return zero, fmt.Errorf("failed to extract structured tool call: %w", extractErr)
	}

	if found {
		// Structured tool was called - return structured result (even if there was an error)
		return StructuredOutputResult[T]{
			HasStructuredOutput: true,
			StructuredResult:    structuredResult,
			TextResponse:        "",
			Messages:            updatedMessages,
		}, nil
	}

	// Tool was not found in messages - check if there was an error
	if err != nil {
		var zero StructuredOutputResult[T]
		return zero, fmt.Errorf("failed to get response from conversation: %w", err)
	}

	// Structured tool was not called - return text response (conversational input)
	return StructuredOutputResult[T]{
		HasStructuredOutput: false,
		StructuredResult:    structuredResult, // zero value
		TextResponse:        textResponse,
		Messages:            updatedMessages,
	}, nil
}

// StructuredOutputResult represents the result of AskWithHistoryStructuredViaTool
// It can contain either structured output (if tool was called) or text response (if tool was not called)
type StructuredOutputResult[T any] struct {
	HasStructuredOutput bool
	StructuredResult    T
	TextResponse        string
	Messages            []llmtypes.MessageContent
}

// parseSchemaForToolParameters parses a JSON schema string and extracts properties for tool parameters
func parseSchemaForToolParameters(schemaString string) (map[string]interface{}, error) {
	var schema map[string]interface{}
	if err := json.Unmarshal([]byte(schemaString), &schema); err != nil {
		return nil, fmt.Errorf("failed to parse schema JSON: %w", err)
	}

	// Extract properties - this becomes the tool parameters
	properties, ok := schema["properties"].(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("schema missing 'properties' field or it's not an object")
	}

	// Build tool parameter schema with type "object"
	toolParams := map[string]interface{}{
		"type":       "object",
		"properties": properties,
	}

	// Add required fields if present
	if required, ok := schema["required"].([]interface{}); ok {
		toolParams["required"] = required
	}

	return toolParams, nil
}

// extractStructuredToolCall scans messages for tool calls matching the tool name and extracts structured data
func extractStructuredToolCall[T any](messages []llmtypes.MessageContent, toolName string) (T, bool, error) {
	var zero T

	// Scan messages in reverse order to find the last (most recent) tool call
	for i := len(messages) - 1; i >= 0; i-- {
		msg := messages[i]

		// Only check AI messages (they contain tool calls)
		if msg.Role != llmtypes.ChatMessageTypeAI {
			continue
		}

		// Check each part for tool calls
		for _, part := range msg.Parts {
			if toolCall, ok := part.(llmtypes.ToolCall); ok {
				if toolCall.FunctionCall != nil && toolCall.FunctionCall.Name == toolName {
					// Found matching tool call - extract arguments
					argsJSON := toolCall.FunctionCall.Arguments
					if argsJSON == "" {
						return zero, false, fmt.Errorf("tool call '%s' has empty arguments", toolName)
					}

					// Parse JSON arguments into struct type T
					var result T
					if err := json.Unmarshal([]byte(argsJSON), &result); err != nil {
						return zero, false, fmt.Errorf("failed to parse tool call arguments: %w", err)
					}

					return result, true, nil
				}
			}
		}
	}

	return zero, false, nil
}

// GetServerNames returns the list of connected server names
func (a *Agent) GetServerNames() []string {
	return getClientNames(a.Clients)
}

// GetContext returns the agent's context for cancellation and lifecycle management
func (a *Agent) GetContext() context.Context {
	return a.ctx
}

// IsCancelled checks if the agent's context has been cancelled
func (a *Agent) IsCancelled() bool {
	return a.ctx.Err() != nil
}

// SetSystemPrompt sets a custom system prompt and marks it as custom to prevent overwriting
// Always overwrites the existing system prompt (removed prepending behavior for code execution mode)
// In code execution mode, if the prompt contains {{TOOL_STRUCTURE}} placeholder, it will be replaced with actual tool structure JSON
func (a *Agent) SetSystemPrompt(systemPrompt string) {
	// Always overwrite the system prompt (removed prepending behavior)
	// In code execution mode, replace {{TOOL_STRUCTURE}} placeholder if present
	if a.UseCodeExecutionMode && strings.Contains(systemPrompt, prompt.ToolStructurePlaceholder) {
		// Get tool structure and replace placeholder
		generatedDir := a.getGeneratedDir()
		toolStructure, err := a.discoverAllServersAndTools(generatedDir)
		if err != nil {
			if a.Logger != nil {
				a.Logger.Warn("âš ï¸ [CODE_EXECUTION] Failed to discover tool structure for placeholder replacement", loggerv2.Error(err))
			}
			// Remove placeholder if discovery fails
			systemPrompt = strings.ReplaceAll(systemPrompt, prompt.ToolStructurePlaceholder, "")
		} else {
			// Replace placeholder with tool structure section
			toolStructureSection := "\n\n<available_code>\n" +
				"**AVAILABLE CODE FILES AND FUNCTIONS:**\n\n" +
				"The following code files and functions are available for use in your Go code. This structure shows all servers, custom tools, and their functions:\n\n" +
				"```json\n" +
				toolStructure + "\n" +
				"```\n\n" +
				"**How to use:**\n" +
				"- Each server has a package name (e.g., \"aws_tools\", \"google_sheets_tools\")\n" +
				"- Each function has a name (e.g., \"GetDocument\", \"ListSpreadsheets\")\n" +
				"- Import the package and call the function in your Go code\n" +
				"</available_code>\n"
			systemPrompt = strings.ReplaceAll(systemPrompt, prompt.ToolStructurePlaceholder, toolStructureSection)
			if a.Logger != nil {
				a.Logger.Info("ðŸ”§ [CODE_EXECUTION] Replaced {{TOOL_STRUCTURE}} placeholder with tool structure", loggerv2.Int("bytes", len(toolStructure)))
			}
		}
	}

	a.SystemPrompt = systemPrompt
	if a.Logger != nil {
		a.Logger.Debug("âœ… System prompt overwritten", loggerv2.Int("length_chars", len(systemPrompt)))
	}
	a.hasCustomSystemPrompt = true
}

// AppendSystemPrompt appends additional content to the existing system prompt
// Removes "AI Staff Engineer" text from existing prompt when appending
func (a *Agent) AppendSystemPrompt(additionalPrompt string) {
	if additionalPrompt == "" {
		return
	}

	// Track the appended prompt for smart routing
	a.AppendedSystemPrompts = append(a.AppendedSystemPrompts, additionalPrompt)
	a.HasAppendedPrompts = true

	// Store original system prompt if this is the first append
	if a.OriginalSystemPrompt == "" {
		a.OriginalSystemPrompt = a.SystemPrompt
	}

	// If we already have a system prompt, remove AI Staff Engineer text and append with separator
	if a.SystemPrompt != "" {
		// Remove "AI Staff Engineer" text from existing prompt before appending
		existingPrompt := prompt.RemoveAIStaffEngineerText(a.SystemPrompt)
		a.SystemPrompt = existingPrompt + "\n\n" + additionalPrompt
		if a.Logger != nil {
			a.Logger.Debug("âœ… System prompt appended - AI Staff Engineer text removed", loggerv2.Int("length_chars", len(additionalPrompt)))
		}
	} else {
		// If no existing system prompt, just set it
		a.SystemPrompt = additionalPrompt
	}

	// Mark as custom to prevent overwriting
	a.hasCustomSystemPrompt = true
}

// RegisterCustomTool registers a single custom tool with both schema and execution function
// category is a REQUIRED parameter that specifies the tool's category (e.g., "workspace", "human", "virtual")
// Returns error if category is missing or empty
func (a *Agent) RegisterCustomTool(name string, description string, parameters map[string]interface{}, executionFunc func(ctx context.Context, args map[string]interface{}) (string, error), category ...string) error {
	if a.customTools == nil {
		a.customTools = make(map[string]CustomTool)
	}

	// Determine category - REQUIRED, no default
	// All tools must have a category from ToolCategories map
	var toolCategory string
	if len(category) > 0 && category[0] != "" {
		toolCategory = category[0]
	} else {
		// Category is required - return error
		err := fmt.Errorf("tool %s registered without category - category is REQUIRED for all tools", name)
		if a.Logger != nil {
			a.Logger.Error("âŒ [DISCOVERY] Tool registered without category", err)
		}
		return err
	}

	// Create the tool definition
	tool := llmtypes.Tool{
		Type: "function",
		Function: &llmtypes.FunctionDefinition{
			Name:        name,
			Description: description,
			Parameters:  llmtypes.NewParameters(parameters),
		},
	}

	// Store both definition and execution function with category
	a.customTools[name] = CustomTool{
		Definition: tool,
		Execution:  executionFunc,
		Category:   toolCategory,
	}

	// ðŸ”§ CRITICAL FIX: Add custom tools to toolToServer mapping with special "custom" marker
	// This ensures they're recognized during tool lookup even when NoServers is used
	if a.toolToServer == nil {
		a.toolToServer = make(map[string]string)
		if a.Logger != nil {
			a.Logger.Debug("ðŸ”§ [TOOL_REGISTRATION] Initialized toolToServer map for custom tools")
		}
	}
	a.toolToServer[name] = "custom"
	if a.Logger != nil {
		a.Logger.Debug(fmt.Sprintf("ðŸ”§ [TOOL_REGISTRATION] Added custom tool '%s' to toolToServer mapping (category: %s)", name, toolCategory))
	}

	// In code execution mode, do NOT add custom tools to LLM tools list
	// They should only be accessible via generated Go code
	// EXCEPTION: Structured output tools (category "structured_output") must always be available
	// because they're orchestration/control tools, not regular MCP tools
	// EXCEPTION: Human tools (category "human") must always be available
	// because they require event bridge access for frontend UI and cannot work via generated code
	isStructuredOutputTool := toolCategory == "structured_output"
	isHumanTool := toolCategory == "human"

	if !a.UseCodeExecutionMode || isStructuredOutputTool || isHumanTool {
		// Normal mode OR structured output tool OR human tool: Add to the main Tools array so the LLM can see it
		a.Tools = append(a.Tools, tool)

		// ðŸ”§ CRITICAL FIX: Also add to filteredTools if smart routing is active
		// This ensures custom tools are available even when smart routing is enabled
		a.filteredTools = append(a.filteredTools, tool)

		if a.UseCodeExecutionMode && isStructuredOutputTool {
			if a.Logger != nil {
				a.Logger.Debug("ðŸ”§ Code execution mode: Structured output tool added to LLM tools (required for orchestration)", loggerv2.String("tool", name))
			}
		}
		if a.UseCodeExecutionMode && isHumanTool {
			if a.Logger != nil {
				a.Logger.Info(fmt.Sprintf("ðŸ”§ Code execution mode: Human tool %s added to LLM tools (requires event bridge for frontend UI) - total tools now: %d", name, len(a.Tools)))
			}
		}
	} else {
		// Code execution mode: Don't add to LLM tools, but still generate code and update registry
		if a.Logger != nil {
			a.Logger.Debug("ðŸ”§ Code execution mode: Custom tool registered but not added to LLM tools (will use generated code)", loggerv2.String("tool", name))
		}
	}

	// Generate Go code for ONLY the new tool (not all tools - that was causing O(nÂ²) regeneration)
	// EXCEPTION: Skip code generation for human tools - they must only be used as direct LLM tools
	// (not via generated code) because they need event bridge access for frontend UI
	if isHumanTool {
		if a.Logger != nil {
			a.Logger.Info(fmt.Sprintf("ðŸ”§ Skipping code generation for human tool %s - must be used as direct LLM tool only", name))
		}
	} else {
		generatedDir := a.getGeneratedDir()
		singleToolForCodeGen := map[string]codegen.CustomToolForCodeGen{
			name: {
				Definition: tool,
				Category:   toolCategory,
			},
		}
		if a.Logger != nil {
			a.Logger.Debug(fmt.Sprintf("ðŸ” [DISCOVERY] Generating code for new tool: %s (category: %s)", name, toolCategory))
		}
		// Use agent's ToolTimeout (same as used for normal tool calls)
		toolTimeout := getToolExecutionTimeout(a)
		if err := codegen.GenerateCustomToolsCode(singleToolForCodeGen, generatedDir, a.Logger, toolTimeout); err != nil {
			if a.Logger != nil {
				a.Logger.Warn(fmt.Sprintf("ðŸ” [DISCOVERY] Failed to generate Go code for tool %s: %v", name, err))
			}
			// Don't fail tool registration if code generation fails
		} else if a.Logger != nil {
			a.Logger.Debug(fmt.Sprintf("ðŸ” [DISCOVERY] Successfully generated code for tool: %s", name))
		}
	}

	// Update registry with new custom tool
	if a.Clients != nil {
		customToolExecutors := make(map[string]func(ctx context.Context, args map[string]interface{}) (string, error))
		for toolName, customTool := range a.customTools {
			customToolExecutors[toolName] = customTool.Execution
		}
		if a.Logger != nil {
			a.Logger.Debug("ðŸ”§ [CODE_EXECUTION] Updating registry with custom tools",
				loggerv2.Int("count", len(customToolExecutors)),
				loggerv2.String("including", name))
			// Log all custom tool names for debugging
			toolNames := make([]string, 0, len(customToolExecutors))
			for toolName := range customToolExecutors {
				toolNames = append(toolNames, toolName)
			}
			a.Logger.Debug("ðŸ”§ [CODE_EXECUTION] Custom tools in registry", loggerv2.Any("tools", toolNames))
		}
		codeexec.InitRegistry(a.Clients, customToolExecutors, a.toolToServer, a.Logger)
		if a.Logger != nil {
			a.Logger.Debug("ðŸ”§ [CODE_EXECUTION] Registry updated successfully for tool", loggerv2.String("tool", name))
		}
	} else {
		if a.Logger != nil {
			a.Logger.Warn("âš ï¸ [CODE_EXECUTION] Cannot update registry - a.Clients is nil for tool", loggerv2.String("tool", name))
		}
	}

	// ðŸ”§ CRITICAL: Rebuild system prompt with updated tool structure in code execution mode
	// This ensures custom tools appear in the system prompt's tool structure JSON
	// so the LLM knows they exist and can use them via generated Go code
	if a.UseCodeExecutionMode {
		if err := a.rebuildSystemPromptWithUpdatedToolStructure(); err != nil {
			if a.Logger != nil {
				a.Logger.Warn("âš ï¸ [CODE_EXECUTION] Failed to rebuild system prompt with updated tool structure", loggerv2.Error(err))
			}
			// Don't fail tool registration if system prompt rebuild fails
		} else {
			if a.Logger != nil {
				a.Logger.Info("âœ… [CODE_EXECUTION] System prompt rebuilt with updated tool structure (custom tool now included)", loggerv2.String("tool", name))
			}
		}
	}

	// Debug logging
	if a.Logger != nil {
		a.Logger.Info("ðŸ”§ Registered custom tool", loggerv2.String("tool", name), loggerv2.String("category", toolCategory))
		a.Logger.Info("ðŸ”§ Total custom tools registered", loggerv2.Int("count", len(a.customTools)))
		a.Logger.Info("ðŸ”§ Total tools in agent", loggerv2.Int("count", len(a.Tools)))
		a.Logger.Info("ðŸ”§ Total filtered tools", loggerv2.Int("count", len(a.filteredTools)))
	}

	return nil
}

// GetCustomToolsByCategory returns all custom tools filtered by category
func (a *Agent) GetCustomToolsByCategory(category string) map[string]CustomTool {
	result := make(map[string]CustomTool)
	for name, tool := range a.customTools {
		if tool.Category == category {
			result[name] = tool
		}
	}
	return result
}

// GetCustomToolCategories returns a list of all unique categories for registered custom tools
func (a *Agent) GetCustomToolCategories() []string {
	categorySet := make(map[string]bool)
	for _, tool := range a.customTools {
		if tool.Category != "" {
			categorySet[tool.Category] = true
		}
	}

	categories := make([]string, 0, len(categorySet))
	for cat := range categorySet {
		categories = append(categories, cat)
	}
	return categories
}

// GetCustomTools returns the registered custom tools
func (a *Agent) GetCustomTools() map[string]CustomTool {
	return a.customTools
}

// UpdateCodeExecutionRegistry explicitly updates the code execution registry with all custom tools
// This is useful when tools are registered after agent initialization (e.g., workspace/human tools)
// It also rebuilds the system prompt to include the newly registered tools in the tool structure
func (a *Agent) UpdateCodeExecutionRegistry() error {
	if a.Clients == nil {
		if a.Logger != nil {
			a.Logger.Warn("âš ï¸ [CODE_EXECUTION] Cannot update registry - a.Clients is nil")
		}
		return fmt.Errorf("cannot update registry: Clients is nil")
	}

	// Build custom tool executors map from all registered custom tools
	customToolExecutors := make(map[string]func(ctx context.Context, args map[string]interface{}) (string, error))
	for toolName, customTool := range a.customTools {
		customToolExecutors[toolName] = customTool.Execution
	}

	if a.Logger != nil {
		a.Logger.Info("ðŸ”§ [CODE_EXECUTION] Explicitly updating registry with custom tools", loggerv2.Int("count", len(customToolExecutors)))
		// Log all custom tool names for debugging
		toolNames := make([]string, 0, len(customToolExecutors))
		for toolName := range customToolExecutors {
			toolNames = append(toolNames, toolName)
		}
		a.Logger.Debug("ðŸ”§ [CODE_EXECUTION] Custom tools being registered", loggerv2.Any("tools", toolNames))
	}

	// Update the registry
	codeexec.InitRegistry(a.Clients, customToolExecutors, a.toolToServer, a.Logger)

	if a.Logger != nil {
		a.Logger.Info("âœ… [CODE_EXECUTION] Registry updated successfully with custom tools", loggerv2.Int("count", len(customToolExecutors)))
	}

	// ðŸ”§ CRITICAL: Rebuild system prompt with updated tool structure in code execution mode
	// This ensures workspace and human tools appear in the system prompt
	if a.UseCodeExecutionMode {
		if err := a.rebuildSystemPromptWithUpdatedToolStructure(); err != nil {
			if a.Logger != nil {
				a.Logger.Warn("âš ï¸ [CODE_EXECUTION] Failed to rebuild system prompt with updated tool structure", loggerv2.Error(err))
			}
			// Don't fail registry update if system prompt rebuild fails
		} else {
			if a.Logger != nil {
				a.Logger.Info("âœ… [CODE_EXECUTION] System prompt rebuilt with updated tool structure (workspace and human tools now included)")
			}
		}
	}

	return nil
}

// rebuildSystemPromptWithUpdatedToolStructure rebuilds the system prompt with the latest tool structure
// This is called after custom tools are registered to ensure they appear in the system prompt
func (a *Agent) rebuildSystemPromptWithUpdatedToolStructure() error {
	if !a.UseCodeExecutionMode {
		return nil // Only needed in code execution mode
	}

	generatedDir := a.getGeneratedDir()
	toolStructure, err := a.discoverAllServersAndTools(generatedDir)
	if err != nil {
		return fmt.Errorf("failed to discover tool structure: %w", err)
	}

	// Rebuild system prompt with updated tool structure
	newSystemPrompt := prompt.BuildSystemPromptWithoutTools(
		a.prompts,
		a.resources,
		string(a.AgentMode),
		a.DiscoverResource,
		a.DiscoverPrompt,
		a.UseCodeExecutionMode,
		toolStructure,
		a.Logger,
	)

	// Update the agent's system prompt
	a.SystemPrompt = newSystemPrompt

	if a.Logger != nil {
		a.Logger.Debug("ðŸ”§ [CODE_EXECUTION] System prompt rebuilt",
			loggerv2.Int("prompt_bytes", len(newSystemPrompt)),
			loggerv2.Int("tool_structure_bytes", len(toolStructure)))
	}

	return nil
}

// GetAppendedSystemPrompts returns the list of appended system prompts
func (a *Agent) GetAppendedSystemPrompts() []string {
	return a.AppendedSystemPrompts
}

// HasAppendedSystemPrompts returns true if any system prompts were appended
func (a *Agent) HasAppendedSystemPrompts() bool {
	return a.HasAppendedPrompts
}

// GetAppendedPromptCount returns the number of appended system prompts
func (a *Agent) GetAppendedPromptCount() int {
	return len(a.AppendedSystemPrompts)
}

// GetAppendedPromptSummary returns a summary of appended prompts
func (a *Agent) GetAppendedPromptSummary() string {
	if !a.HasAppendedPrompts || len(a.AppendedSystemPrompts) == 0 {
		return ""
	}

	var summary strings.Builder
	for i, prompt := range a.AppendedSystemPrompts {
		if i > 0 {
			summary.WriteString("; ")
		}
		content := prompt
		if len(content) > 100 {
			content = content[:100] + "..."
		}
		summary.WriteString(content)
	}
	return summary.String()
}

// getGeneratedDir returns the path to the generated/ directory
// Only creates the directory if code execution mode is enabled
func (a *Agent) getGeneratedDir() string {
	// Use shared utility for path calculation (single source of truth)
	path := mcpcache.GetGeneratedDirPath()

	// Only create directory if code execution mode is enabled
	// In simple agent mode, we don't need the generated directory
	if a.UseCodeExecutionMode {
		_ = mcpcache.EnsureGeneratedDir(path, a.Logger)
	}

	return path
}
