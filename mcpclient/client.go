package mcpclient

import (
	"context"
	"fmt"
	"math"
	"os"
	"strings"
	"sync"
	"time"

	loggerv2 "mcpagent/logger/v2"

	"github.com/mark3labs/mcp-go/client"
	"github.com/mark3labs/mcp-go/mcp"
)

// RetryConfig defines the retry behavior for MCP connections
type RetryConfig struct {
	MaxRetries     int           // Maximum number of retry attempts
	InitialDelay   time.Duration // Initial delay between retries
	MaxDelay       time.Duration // Maximum delay between retries
	BackoffFactor  float64       // Exponential backoff multiplier
	ConnectTimeout time.Duration // Timeout for each individual connection attempt
}

// DefaultRetryConfig returns a sensible default retry configuration
func DefaultRetryConfig() RetryConfig {
	return RetryConfig{
		MaxRetries:     3,
		InitialDelay:   1 * time.Second,
		MaxDelay:       30 * time.Second,
		BackoffFactor:  2.0,
		ConnectTimeout: 15 * time.Minute, // Increased from 10 minutes to 15 minutes for very slow npx commands
	}
}

// Client wraps the underlying MCP client with convenience methods
type Client struct {
	config        MCPServerConfig
	mcpClient     *client.Client
	serverInfo    *mcp.Implementation
	retryConfig   RetryConfig
	logger        loggerv2.Logger
	contextCancel context.CancelFunc // Store context cancel function for SSE connections
	context       context.Context    // Store context for SSE connections
	mu            sync.RWMutex       // Protect access to contextCancel and context
}

// New creates a new MCP client for the given server configuration
func New(config MCPServerConfig, logger loggerv2.Logger) *Client {
	return &Client{
		config:      config,
		retryConfig: DefaultRetryConfig(),
		logger:      logger,
	}
}

// NewWithRetryConfig creates a new MCP client with custom retry configuration
func NewWithRetryConfig(config MCPServerConfig, retryConfig RetryConfig, logger loggerv2.Logger) *Client {
	return &Client{
		config:      config,
		retryConfig: retryConfig,
		logger:      logger,
	}
}

// Connect establishes a connection to the MCP server with retry logic
func (c *Client) Connect(ctx context.Context) error {
	maxRetries := 3
	baseDelay := time.Second

	for attempt := 1; attempt <= maxRetries; attempt++ {
		if attempt > 1 {
			delay := time.Duration(attempt-1) * baseDelay
			c.logger.Debug("Retrying MCP connection",
				loggerv2.Int("attempt", attempt),
				loggerv2.Int("max_retries", maxRetries),
				loggerv2.String("server", c.getServerName()),
				loggerv2.String("delay", delay.String()))
			time.Sleep(delay)
		}

		protocol := c.config.GetProtocol()
		if protocol == ProtocolStdio {
			c.logger.Debug("Connecting to MCP server",
				loggerv2.String("server", c.getServerName()),
				loggerv2.String("protocol", string(protocol)),
				loggerv2.String("command", c.config.Command),
				loggerv2.Any("args", c.config.Args))
		} else {
			c.logger.Debug("Connecting to MCP server",
				loggerv2.String("server", c.getServerName()),
				loggerv2.String("protocol", string(protocol)),
				loggerv2.String("url", c.config.URL))
		}

		err := c.connectOnce(ctx)
		if err == nil {
			if attempt > 1 {
				c.logger.Info("Successfully connected to MCP server after retry attempts",
					loggerv2.String("server", c.getServerName()),
					loggerv2.Int("retry_attempts", attempt-1))
			} else {
				c.logger.Info("Successfully connected to MCP server on first attempt",
					loggerv2.String("server", c.getServerName()))
			}
			return nil
		}

		c.logger.Error("Connection attempt failed", err,
			loggerv2.String("server", c.getServerName()),
			loggerv2.Int("attempt", attempt))

		if attempt == maxRetries {
			return fmt.Errorf("failed to connect to MCP server '%s' after %d attempts: %w", c.getServerName(), maxRetries, err)
		}
	}

	return fmt.Errorf("unexpected error in retry loop for server '%s'", c.getServerName())
}

// connectOnce performs a single connection attempt
func (c *Client) connectOnce(ctx context.Context) error {
	// Prepare environment variables
	// Start with the current process environment, then override with config env vars
	env := os.Environ()

	// Create a map of existing env vars for quick lookup
	envMap := make(map[string]string)
	for _, e := range env {
		if idx := strings.IndexByte(e, '='); idx > 0 {
			key := e[:idx]
			value := e[idx+1:]
			envMap[key] = value
		}
	}

	// Override with config env vars
	for key, value := range c.config.Env {
		envMap[key] = value
	}

	// Convert back to []string format
	env = make([]string, 0, len(envMap))
	for key, value := range envMap {
		env = append(env, fmt.Sprintf("%s=%s", key, value))
	}

	var mcpClient *client.Client
	var err error

	// Create MCP client based on protocol type (use smart detection)
	protocol := c.config.GetProtocol()
	switch protocol {
	case ProtocolSSE:
		// Use SSE transport
		sseManager := NewSSEManager(c.config.URL, c.config.Headers, c.logger)
		mcpClient, err = sseManager.Connect(ctx)
		if err != nil {
			return fmt.Errorf("failed to create SSE MCP client: %w", err)
		}

	case ProtocolHTTP:
		// Use HTTP transport
		httpManager := NewHTTPManager(c.config.URL, c.config.Headers, c.logger)
		mcpClient, err = httpManager.Connect(ctx)
		if err != nil {
			return fmt.Errorf("failed to create HTTP MCP client: %w", err)
		}

	case ProtocolStdio:
		fallthrough
	default:
		// Default to stdio for backward compatibility
		stdioManager := NewStdioManager(c.config.Command, c.config.Args, env, c.logger)
		mcpClient, err = stdioManager.Connect(ctx)
		if err != nil {
			return fmt.Errorf("failed to create MCP client: %w", err)
		}
	}

	c.mcpClient = mcpClient

	// For stdio clients, initialization is handled by the transport manager
	// For other protocols, we need to initialize here
	if protocol != ProtocolStdio {
		// Initialize connection
		initResult, err := c.mcpClient.Initialize(ctx, mcp.InitializeRequest{
			Params: mcp.InitializeParams{
				ProtocolVersion: "2024-11-05",
				Capabilities:    mcp.ClientCapabilities{},
				ClientInfo: mcp.Implementation{
					Name:    "mcp-agent-go",
					Version: "1.0.0",
				},
			},
		})
		if err != nil {
			_ = c.mcpClient.Close() // Ignore errors during cleanup
			return fmt.Errorf("failed to initialize MCP connection: %w", err)
		}

		c.serverInfo = &initResult.ServerInfo
	} else {
		// For stdio, we need to get server info separately since initialization was already done
		// We'll get this from the first tool listing or other operation
		c.serverInfo = &mcp.Implementation{
			Name:    "stdio-server",
			Version: "1.0.0",
		}
	}

	return nil
}

// ConnectWithRetry establishes connection to the MCP server with retry logic
func (c *Client) ConnectWithRetry(ctx context.Context) error {
	var lastErr error

	for attempt := 0; attempt <= c.retryConfig.MaxRetries; attempt++ {
		if attempt > 0 {
			// Calculate delay with exponential backoff
			delay := time.Duration(float64(c.retryConfig.InitialDelay) * math.Pow(c.retryConfig.BackoffFactor, float64(attempt-1)))
			if delay > c.retryConfig.MaxDelay {
				delay = c.retryConfig.MaxDelay
			}

			c.logger.Debug("Retrying MCP connection",
				loggerv2.Int("attempt", attempt+1),
				loggerv2.Int("max_retries", c.retryConfig.MaxRetries+1),
				loggerv2.String("server", c.getServerName()),
				loggerv2.String("delay", delay.String()))

			select {
			case <-time.After(delay):
				// Continue with retry
			case <-ctx.Done():
				return fmt.Errorf("context cancelled during retry delay: %w", ctx.Err())
			}
		}

		// Create context with timeout for this specific attempt
		connectCtx, cancel := context.WithTimeout(ctx, c.retryConfig.ConnectTimeout)

		// Log connection attempt
		if attempt == 0 {
			c.logger.Debug("Connecting to MCP server",
				loggerv2.String("server", c.getServerName()),
				loggerv2.String("command", c.config.Command),
				loggerv2.Any("args", c.config.Args))
		}

		// Attempt connection
		err := c.Connect(connectCtx)
		cancel()

		if err == nil {
			if attempt > 0 {
				c.logger.Info("Successfully connected to MCP server after retry attempts",
					loggerv2.String("server", c.getServerName()),
					loggerv2.Int("retry_attempts", attempt))
			} else {
				c.logger.Info("Successfully connected to MCP server on first attempt",
					loggerv2.String("server", c.getServerName()))
			}
			return nil
		}

		lastErr = err
		c.logger.Error("Connection attempt failed", err,
			loggerv2.String("server", c.getServerName()),
			loggerv2.Int("attempt", attempt+1))

		// If this was the last attempt, don't sleep
		if attempt == c.retryConfig.MaxRetries {
			break
		}

		// Check if context was cancelled
		if ctx.Err() != nil {
			return fmt.Errorf("context cancelled during connection retry: %w", ctx.Err())
		}
	}

	return fmt.Errorf("failed to connect to MCP server '%s' after %d attempts: %w",
		c.getServerName(), c.retryConfig.MaxRetries+1, lastErr)
}

// getServerName returns a human-readable name for the server (used for logging)
func (c *Client) getServerName() string {
	if c.config.Description != "" {
		return c.config.Description
	}
	return fmt.Sprintf("%s %v", c.config.Command, c.config.Args)
}

// Close closes the connection to the MCP server
func (c *Client) Close() error {
	// For SSE connections, cancel the stored context first
	if c.contextCancel != nil {
		c.logger.Debug("Canceling SSE context before closing client")
		c.contextCancel()
	}

	// Clear the stored context and cancel function
	c.mu.Lock()
	c.context = nil
	c.contextCancel = nil
	c.mu.Unlock()

	if c.mcpClient != nil {
		return c.mcpClient.Close()
	}
	return nil
}

// GetServerInfo returns information about the connected server
func (c *Client) GetServerInfo() *mcp.Implementation {
	return c.serverInfo
}

// GetMCPClient returns the underlying MCP client (for pooled client usage)
func (c *Client) GetMCPClient() *client.Client {
	return c.mcpClient
}

// ListTools returns all available tools from the server
func (c *Client) ListTools(ctx context.Context) ([]mcp.Tool, error) {
	c.logger.Debug("Starting ListTools call")

	if c.mcpClient == nil {
		c.logger.Debug("Client not connected")
		return nil, fmt.Errorf("client not connected")
	}

	c.logger.Debug("About to call underlying mcpClient.ListTools")
	deadline, hasDeadline := ctx.Deadline()
	c.logger.Debug("Context info",
		loggerv2.Any("has_deadline", hasDeadline),
		loggerv2.Any("deadline", deadline),
		loggerv2.Any("done", ctx.Done()))

	listStartTime := time.Now()

	// Call ListTools directly without goroutine wrapper
	c.logger.Debug("About to make the actual ListTools call")

	// Add a timeout wrapper to see if it's the call itself
	callCtx, callCancel := context.WithTimeout(ctx, 5*time.Minute)
	defer callCancel()

	c.logger.Debug("Making ListTools call with 5m timeout")
	result, err := c.mcpClient.ListTools(callCtx, mcp.ListToolsRequest{})

	if err != nil {
		c.logger.Debug("ListTools call returned with error", loggerv2.Error(err))
	} else {
		c.logger.Debug("ListTools call returned successfully")
	}

	listDuration := time.Since(listStartTime)
	c.logger.Debug("ListTools call completed",
		loggerv2.String("duration", listDuration.String()))

	if err != nil {
		c.logger.Error("Failed to list tools", err)
		return nil, fmt.Errorf("failed to list tools: %w", err)
	}

	c.logger.Debug("Successfully listed tools", loggerv2.Int("tool_count", len(result.Tools)))
	return result.Tools, nil
}

// CallTool invokes a tool with the given arguments
func (c *Client) CallTool(ctx context.Context, name string, arguments map[string]interface{}) (*mcp.CallToolResult, error) {
	if c.mcpClient == nil {
		return nil, fmt.Errorf("client not connected")
	}

	result, err := c.mcpClient.CallTool(ctx, mcp.CallToolRequest{
		Params: mcp.CallToolParams{
			Name:      name,
			Arguments: arguments,
		},
	})
	if err != nil {
		return nil, fmt.Errorf("failed to call tool %s: %w", name, err)
	}

	return result, nil
}

// ListResources lists all available resources from the server
func (c *Client) ListResources(ctx context.Context) ([]mcp.Resource, error) {
	c.logger.Debug("ListResources: Starting call")

	if c.mcpClient == nil {
		c.logger.Debug("ListResources: Client not connected")
		return nil, fmt.Errorf("client not connected")
	}

	c.logger.Debug("ListResources: Calling mcpClient.ListResources")
	result, err := c.mcpClient.ListResources(ctx, mcp.ListResourcesRequest{})
	if err != nil {
		c.logger.Error("ListResources: Error from mcpClient", err)
		return nil, fmt.Errorf("failed to list resources: %w", err)
	}

	resourceCount := 0
	if result != nil {
		resourceCount = len(result.Resources)
		c.logger.Debug("ListResources: Successfully received result",
			loggerv2.Int("resource_count", resourceCount))

		// Log resource details if any are found
		if resourceCount > 0 {
			for i, resource := range result.Resources {
				c.logger.Debug("ListResources: Resource details",
					loggerv2.Int("index", i),
					loggerv2.String("uri", resource.URI),
					loggerv2.String("name", resource.Name),
					loggerv2.String("description", resource.Description),
					loggerv2.String("mime_type", resource.MIMEType))
			}
		} else {
			c.logger.Debug("ListResources: Result received but Resources slice is empty")
		}
	} else {
		c.logger.Debug("ListResources: Result is nil")
	}

	return result.Resources, nil
}

// GetResource gets a specific resource by URI
func (c *Client) GetResource(ctx context.Context, uri string) (*mcp.ReadResourceResult, error) {
	if c.mcpClient == nil {
		return nil, fmt.Errorf("client not connected")
	}

	result, err := c.mcpClient.ReadResource(ctx, mcp.ReadResourceRequest{
		Params: mcp.ReadResourceParams{
			URI: uri,
		},
	})
	if err != nil {
		return nil, fmt.Errorf("failed to get resource %s: %w", uri, err)
	}

	return result, nil
}

// ListPrompts lists all available prompts from the server
func (c *Client) ListPrompts(ctx context.Context) ([]mcp.Prompt, error) {
	if c.mcpClient == nil {
		return nil, fmt.Errorf("client not connected")
	}

	result, err := c.mcpClient.ListPrompts(ctx, mcp.ListPromptsRequest{})
	if err != nil {
		return nil, fmt.Errorf("failed to list prompts: %w", err)
	}

	return result.Prompts, nil
}

// GetPrompt gets a specific prompt by name
func (c *Client) GetPrompt(ctx context.Context, name string) (*mcp.GetPromptResult, error) {
	if c.mcpClient == nil {
		return nil, fmt.Errorf("client not connected")
	}

	// Create the MCP request
	request := mcp.GetPromptRequest{
		Params: mcp.GetPromptParams{
			Name: name,
		},
	}

	// Send the request
	response, err := c.mcpClient.GetPrompt(ctx, request)
	if err != nil {
		return nil, fmt.Errorf("failed to get prompt: %w", err)
	}

	return response, nil
}

// SetContextCancel stores the context cancel function for later cleanup (used for SSE connections)
func (c *Client) SetContextCancel(cancel context.CancelFunc) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.contextCancel = cancel
}

// GetContextCancel retrieves the stored context cancel function
func (c *Client) GetContextCancel() context.CancelFunc {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return c.contextCancel
}

// SetContext stores the context for SSE connections
func (c *Client) SetContext(ctx context.Context) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.context = ctx
}

// GetContext retrieves the stored context
func (c *Client) GetContext() context.Context {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return c.context
}

// ConnectWithTimeout is a convenience method that connects with a default timeout
func (c *Client) ConnectWithTimeout(timeout time.Duration) error {
	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	defer cancel()
	return c.ConnectWithRetry(ctx)
}

// ParallelToolDiscoveryResult represents the result of discovering tools from a single server
type ParallelToolDiscoveryResult struct {
	ServerName string
	Tools      []mcp.Tool
	Error      error
	Client     ClientInterface // Add client to the result so it can be reused
}

// DiscoverAllToolsParallel connects to all servers in the config in parallel, lists tools, and returns results per server.
func DiscoverAllToolsParallel(ctx context.Context, cfg *MCPConfig, logger loggerv2.Logger) []ParallelToolDiscoveryResult {
	servers := cfg.ListServers()
	if len(servers) == 0 {
		logger.Debug("No servers configured, returning empty result")
		return []ParallelToolDiscoveryResult{}
	}

	logger.Info("DiscoverAllToolsParallel started",
		loggerv2.Int("server_count", len(servers)),
		loggerv2.Any("servers", servers))
	logger.Debug("Parent context info",
		loggerv2.Any("done", ctx.Done()),
		loggerv2.Any("err", ctx.Err()))

	resultsCh := make(chan ParallelToolDiscoveryResult, len(servers))
	var wg sync.WaitGroup

	logger.Debug("Starting goroutines for servers", loggerv2.Int("count", len(servers)))
	for _, name := range servers {
		srvCfg, _ := cfg.GetServer(name) // ignore error, will be caught below
		wg.Add(1)
		logger.Debug("Starting goroutine for server",
			loggerv2.String("server", name),
			loggerv2.String("protocol", string(srvCfg.Protocol)))
		go func(name string, srvCfg MCPServerConfig) {
			// Ensure we always send a result and call wg.Done, even if we panic
			resultSent := false
			defer func() {
				if !resultSent {
					logger.Warn("Goroutine exiting without sending result, sending error result",
						loggerv2.String("server", name))
					// Send error result to prevent deadlock
					select {
					case resultsCh <- ParallelToolDiscoveryResult{
						ServerName: name,
						Tools:      nil,
						Error:      fmt.Errorf("goroutine exited unexpectedly without sending result"),
						Client:     nil,
					}:
						resultSent = true
					default:
						// Channel might be closed or full, but we tried
						logger.Error("Failed to send error result, channel may be closed", nil,
							loggerv2.String("server", name))
					}
				}
				wg.Done()
			}()

			logger.Debug("Goroutine started for server", loggerv2.String("server", name))

			// Check parent context cancellation before starting connection
			// This prevents creating isolated contexts when parent is already cancelled
			select {
			case <-ctx.Done():
				logger.Warn("Parent context cancelled before starting connection, skipping server",
					loggerv2.String("server", name),
					loggerv2.Error(ctx.Err()))
				resultsCh <- ParallelToolDiscoveryResult{
					ServerName: name,
					Tools:      nil,
					Error:      fmt.Errorf("parent context cancelled: %w", ctx.Err()),
					Client:     nil,
				}
				resultSent = true
				return
			default:
				// Parent context still valid, continue
			}

			client := New(srvCfg, logger)
			var cancel context.CancelFunc
			var connCtx context.Context

			if srvCfg.Protocol == ProtocolSSE {
				// For SSE, create a new background context with timeout to avoid parent cancellation
				// IMPORTANT: Do NOT defer cancel() here - we need the context to remain valid for the entire client lifecycle
				connCtx, cancel = context.WithTimeout(context.Background(), 15*time.Minute)
				logger.Debug("Using SSE protocol with isolated context",
					loggerv2.String("server", name),
					loggerv2.String("timeout", "15m"))
			} else {
				// For stdio and other protocols, also use isolated context with longer timeout
				connCtx, cancel = context.WithTimeout(context.Background(), 15*time.Minute)
				defer cancel() // Safe to cancel immediately for non-SSE protocols
				logger.Debug("Using protocol with isolated context",
					loggerv2.String("protocol", string(srvCfg.Protocol)),
					loggerv2.String("server", name),
					loggerv2.String("timeout", "15m"))
			}

			logger.Info("Connecting to MCP server", loggerv2.String("server", name))
			connectStartTime := time.Now()

			if err := client.ConnectWithRetry(connCtx); err != nil {
				connectDuration := time.Since(connectStartTime)
				logger.Error("Connection failed", err,
					loggerv2.String("server", name),
					loggerv2.String("duration", connectDuration.String()))
				if cancel != nil {
					cancel() // Clean up context on connection failure
				}
				resultsCh <- ParallelToolDiscoveryResult{ServerName: name, Tools: nil, Error: err, Client: nil}
				resultSent = true
				return
			}

			connectDuration := time.Since(connectStartTime)
			logger.Info("Connection successful",
				loggerv2.String("server", name),
				loggerv2.String("duration", connectDuration.String()))

			// For SSE connections, the SSE manager now uses background context for Start() automatically
			// For other protocols, no additional Start() call is needed
			logger.Debug("Client ready for use", loggerv2.String("server", name))

			// For SSE connections, use the same isolated context for tool listing
			// For other protocols, use the same isolated context
			listCtx := connCtx // Use the same isolated context for all protocols

			// Check parent context cancellation before tool listing
			// This allows early exit if parent context is cancelled
			select {
			case <-ctx.Done():
				logger.Warn("Parent context cancelled before tool listing, aborting",
					loggerv2.String("server", name),
					loggerv2.Error(ctx.Err()))
				if cancel != nil {
					cancel() // Clean up context
				}
				resultsCh <- ParallelToolDiscoveryResult{
					ServerName: name,
					Tools:      nil,
					Error:      fmt.Errorf("parent context cancelled during tool listing: %w", ctx.Err()),
					Client:     nil,
				}
				resultSent = true
				return
			default:
				// Parent context still valid, continue
			}

			logger.Debug("Starting tool listing for server", loggerv2.String("server", name))
			logger.Debug("Context info before ListTools",
				loggerv2.String("server", name),
				loggerv2.Any("context_done", listCtx.Done()),
				loggerv2.Any("context_err", listCtx.Err()))
			listStartTime := time.Now()

			logger.Debug("Calling client.ListTools for server", loggerv2.String("server", name))
			tools, err := client.ListTools(listCtx)

			listDuration := time.Since(listStartTime)
			logger.Debug("ListTools completed",
				loggerv2.String("server", name),
				loggerv2.String("duration", listDuration.String()),
				loggerv2.Error(err))

			// Don't close the client here - we need to reuse it for agent creation
			// _ = client.Close()

			// For SSE connections, store the context and cancel function for later cleanup
			// Don't cancel the context here - it needs to remain valid for the client lifecycle
			if srvCfg.Protocol == ProtocolSSE {
				// Store the context and cancel function in the client for later cleanup
				// We'll cancel it when the client is actually closed
				client.SetContextCancel(cancel)
				client.SetContext(connCtx) // Store the context as well
				logger.Debug("Stored SSE context and cancel function for later cleanup",
					loggerv2.String("server", name))
			}

			if err != nil {
				logger.Error("Tool listing failed", err, loggerv2.String("server", name))
			} else {
				logger.Info("Tool listing successful",
					loggerv2.String("server", name),
					loggerv2.Int("tools_count", len(tools)))
			}

			logger.Debug("Sending result for server", loggerv2.String("server", name))
			resultsCh <- ParallelToolDiscoveryResult{ServerName: name, Tools: tools, Error: err, Client: client}
			resultSent = true
			logger.Debug("Result sent for server", loggerv2.String("server", name))
		}(name, srvCfg)
	}

	results := make([]ParallelToolDiscoveryResult, 0, len(servers))
	received := make(map[string]bool)
	total := len(servers)

	logger.Debug("Starting result collection loop", loggerv2.Int("total", total))
	timeout := false
	done := make(chan struct{})
	go func() {
		wg.Wait()
		logger.Debug("All goroutines finished, closing done channel")
		close(done)
	}()

	// Add a maximum timeout for result collection (30 minutes to allow for retries)
	// This prevents infinite waiting if goroutines get stuck
	resultCollectionTimeout := 30 * time.Minute
	resultCollectionCtx, resultCollectionCancel := context.WithTimeout(ctx, resultCollectionTimeout)
	defer resultCollectionCancel()

	resultCollectionStartTime := time.Now()
	allGoroutinesDone := false
	for receivedCount := 0; receivedCount < total && !timeout && !allGoroutinesDone; {
		logger.Debug("Waiting for results",
			loggerv2.Int("received", receivedCount),
			loggerv2.Int("total", total))
		select {
		case r := <-resultsCh:
			results = append(results, r)
			received[r.ServerName] = true
			receivedCount++
			logger.Debug("Received result for server",
				loggerv2.String("server", r.ServerName),
				loggerv2.Int("total_received", receivedCount),
				loggerv2.Int("total", total))
		case <-resultCollectionCtx.Done():
			logger.Warn("Result collection timeout or parent context cancelled, stopping result collection",
				loggerv2.String("reason", resultCollectionCtx.Err().Error()))
			timeout = true
		case <-done:
			logger.Debug("All goroutines finished, draining remaining results")
			allGoroutinesDone = true
			// Drain any remaining results before breaking
			drained := false
			for !drained {
				select {
				case r := <-resultsCh:
					results = append(results, r)
					received[r.ServerName] = true
					receivedCount++
					logger.Debug("Drained result for server",
						loggerv2.String("server", r.ServerName),
						loggerv2.Int("total_received", receivedCount))
				default:
					drained = true
				}
			}
		}
	}

	resultCollectionDuration := time.Since(resultCollectionStartTime)
	logger.Debug("Result collection completed",
		loggerv2.String("duration", resultCollectionDuration.String()),
		loggerv2.Any("timeout", timeout),
		loggerv2.Int("received", len(results)),
		loggerv2.Int("total", total))

	// If timeout, add missing servers as timeouts
	if timeout {
		logger.Warn("Timeout detected, adding missing servers as timeouts")
		for _, name := range servers {
			if !received[name] {
				logger.Warn("Adding timeout result for missing server", loggerv2.String("server", name))
				results = append(results, ParallelToolDiscoveryResult{
					ServerName: name,
					Tools:      nil,
					Error:      fmt.Errorf("tool discovery timed out for this server"),
				})
			}
		}
	}

	// Drain any remaining results (if any)
	for len(results) < total {
		select {
		case r := <-resultsCh:
			results = append(results, r)
		default:
			// No more results available
		}
	}

	// Emit comprehensive cache event for all discovered servers
	// This ensures the frontend can see comprehensive cache status during active operations
	serverNames := make([]string, 0, len(servers))
	serverStatus := make(map[string]interface{})

	for _, result := range results {
		serverNames = append(serverNames, result.ServerName)
		status := "ok"
		if result.Error != nil {
			status = "error"
		}
		serverStatus[result.ServerName] = map[string]interface{}{
			"status":      status,
			"tools_count": len(result.Tools),
			"error":       result.Error,
		}
	}

	// Log comprehensive cache event for debugging
	logger.Debug("Comprehensive cache event for active tool discovery",
		loggerv2.Int("servers_count", len(serverNames)),
		loggerv2.Any("servers", serverNames),
		loggerv2.Int("total_tools", len(results)))

	// Final summary logging
	successCount := 0
	errorCount := 0
	totalTools := 0
	for _, result := range results {
		if result.Error == nil {
			successCount++
			totalTools += len(result.Tools)
		} else {
			errorCount++
		}
	}

	logger.Info("FINAL SUMMARY",
		loggerv2.Int("total_servers", len(results)),
		loggerv2.Int("successful", successCount),
		loggerv2.Int("failed", errorCount),
		loggerv2.Int("total_tools", totalTools))

	// Note: To emit actual events, we would need to pass tracers to this function
	// For now, we log the information so it appears in the server logs

	return results
}
